% Copyright (C) 2014-2025 by Thomas Auzinger <thomas@auzinger.name>

% Inside the square brackets (e.g., [draft,onside]), different options can be chosen:
% 'draft': Obtain debug information on the build.
% 'final': Remove all debug information in the final build.
% 'onside': Create document for onsided printing (without empty pages added by LaTeX).
% 'twoside': Create document for twosided printing, which adds empty pages to start chapters on odd pages.
\documentclass[final,twoside]{vutinfth}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Moritz Anton Zideck} % The author name without titles.
\newcommand{\thesistitle}{Impact of Temporal Context on Robustness in UAV-based Imagery} % The title of the thesis. The English version should be used, if it exists.

% Create the XMP metadata file for the creation of PDF/A compatible documents.
\begin{filecontents*}[overwrite]{\jobname.xmpdata}
\Author{\authorname}                                    % The author's name in the document properties.
\Title{\thesistitle}                                    % The document's title in the document properties.
\Language{en-GB}                                        % The document's language in the document properties. Select 'en-US', 'en-GB', or 'de-AT'.
\Keywords{a\sep list\sep of\sep keywords}               % The document's keywords in the document properties (separated by '\sep ').
\Publisher{TU Wien}                                     % The document's publisher in the document properties.
\Subject{Thesis}                                        % The document's subject in the document properties.
\end{filecontents*}

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesetting of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}        % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes}  % Provides tooltip-like todo notes.
\usepackage{morewrites} % Increases the number of external files that can be used.
\usepackage[a-2b,mathxmp]{pdfx}      % Enables PDF/A compliance. Loads the package hyperref and has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists of acronyms. This package has to be included last.
\usepackage{pgfplots}
\usepackage{tikz}
\pgfplotsset{compat=1.18}
\usepgfplotslibrary{groupplots}
\usepackage{caption} % for \captionsetup if you want
\usepackage{float}
\usepackage{colortbl}
\usepackage{makecell}
% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around hyperlinks (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph indentation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setadvisor{Senior Lecturer Dipl.-Ing. Dr.techn.}{Sebastian Zambanini}{}{male}

% For bachelor and master theses:
\setfirstassistant{Dipl. Inf}{Marvin Burges}{}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{12217036}
\setdate{01}{01}{2001} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{Auswirkungen des zeitlichen Kontexts auf die Robustheit in der UAV-gestützten Bildverarbeitung} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor.
% Bachelor:
\setthesis{bachelor}
%
% Master:
%\setthesis{master}
%\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.

% For bachelor and master:
\setcurriculum{Media Informatics and Visual Computing}{Medieninformatik und Visual Computing} % Sets the English and German name of the curriculum.

% Optional reviewer data:
\setfirstreviewerdata{Affiliation, Country}
\setsecondreviewerdata{Affiliation, Country}

% Define colors
\definecolor{tableheader}{RGB}{220,220,220}
\definecolor{tablerow}{RGB}{245,245,245}



\begin{document}

\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

\addtitlepage{naustrian} % German title page.
\addtitlepage{english} % English title page.
\addstatementpage{naustrian}  % Select 'english' for the English statement text.

\begin{danksagung*}
\todo{Ihr Text hier.}
\end{danksagung*}

\begin{acknowledgements*}
\todo{Enter your text here.}
\end{acknowledgements*}

\begin{kurzfassung}
\todo{Ihr Text hier.}
\end{kurzfassung}

\begin{abstract}
\todo{Enter your text here.}
\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

\chapter{Introduction}
Video object detection is a relatively new field in computer vision that aims to leverage the temporal context of videos in order to improve detection performance compared to image-based object detection.
Temporal context refers to information that can be extracted from the time domain of a video, such as object motion, object trajectories, and changes in object appearance over time.
By exploiting this additional information, video object detection models can achieve higher accuracy as well as provide more stable detections for tasks like Tracking.
This is especially important in scenarios where objects may be occluded, blurred, or undergo significant appearance changes over time.
A prominent example is UAV-based imagery, where the camera is constantly moving, objects are often small, and visual conditions can change rapidly, making reliable object detection particularly challenging.

To date, most evaluations of object detection models—both image-based and video-based—focus primarily on accuracy measured on clean and unperturbed data.
However, in real-world deployments visual data is frequently affected by a wide range of perturbations, including sensor noise, compression artifacts, motion blur, illumination changes, and variations in viewpoint and scale.
These perturbations can substantially degrade detection performance.
Although several works have investigated the robustness of image-based object detectors to such corruptions, only limited attention has been given to the robustness of video object detection models, and virtually no systematic studies exist for UAV-based video data in particular.

In this thesis, the focus is therefore placed on the robustness of video object detection models under common perturbations in UAV-based imagery.
Robustness is defined as the ability of a model to maintain its detection performance when exposed to adverse conditions such as lighting changes, weather effects, motion blur, occlusions, and variations in object appearance and scale.
A model may achieve high accuracy on clean data, yet still be unreliable in practice if its performance deteriorates significantly under realistic perturbations.
For safety-critical and autonomous UAV applications, such robustness is essential.

Therefore, the main research question of this thesis is formulated as follows:

\begin{description}
    \item[RQ1:] To what extent does temporal context influence the robustness of video object detection models under varying environmental and perturbation conditions on the VisDrone dataset?
    \begin{enumerate}
        \item[RQ1.1:] How does the amount of temporal context (i.e., number of reference frames) used by video object detection models affect their robustness to different types of perturbations compared to single-frame baselines?
        \item[RQ1.2:] How do different types of perturbations (e.g., motion blur, noise, brightness changes) impact the detection performance of video object detection models with varying amounts of temporal context?
        \item[RQ1.3:] Can video object detection models use temporal context to improve detection performance on single frames that are heavily perturbed, by leveraging information from adjacent unperturbed frames?
    \end{enumerate}
\end{description}

By answering these research questions, this thesis aims to provide a deeper insight into video detection models' capabilities and limitations, to highlight the importance of temporal context for robustness in UAV-based imagery, and to identify potential avenues for future research to enhance the reliability of video object detection systems in real-world applications.


\section{Related Work}
This chapter provides an overview of existing work related to the topic of video object detection and robustness in computer vision.
First i will give a brief overview of the state of the art in object detection, that serves as a foundation for video object detection.
After that i will highlight the most relevant works in video object detection, like FGFA \cite{zhu2017fgfa} and MEGA \cite{chen2018mega}, as well as the two models used in this thesis, TransVOD \cite{zhu2021transvod} and YOLOV \cite{liu2023yolov}.
Finally i will discuss existing works on robustness evaluation in computer vision, including benchmarks like ImageNet-C \cite{hendrycks2019benchmarking} and studies on the robustness of object detection models in UAV-based imagery \cite{liu2024UAVRobustness}.

\subsection{Object Detection}
In the research field of computer vision, object detection is defined as the task of \textbf{identifying and localizing objects of interest within an image or video frame}. 
This problem was traditionally approached using hand-crafted features and classical machine learning techniques, such as Haar cascades \cite{viola2001rapidobjectdetection} and HOG + SVM \cite{dalal2005histogramoforientedgradients}.
However, with the advent of deep learning, particularly convolutional neural networks (CNNs), object detection has seen significant advancements.
Since then multiple differnt architectures have emerged, each with its own strengths and weaknesses.
In regards to this thesis, two main categories of object detection are relevant: transformer-based detectors and one-stage detectors.

Transformers, like \textbf{DETR} \cite{carion2020endtoendobjectdetectiontransformers} and \textbf{Deformable DETR} \cite{zhu2021deformabledetr}, use self-attention meachnisms, which makes them able to capture long-range dependencies in the input data.
This feature is particularly useful for object detection, as it allows the model to consider the entire image context when making predictions.
Therefore transformer-based detectors have shown strong performance on various object detection benchmarks, often outperforming traditional CNN-based methods, however they tend to be computationally more expensive and require larger amounts of training data.

On the other hand one-stage detectors, like \textbf{YOLO} \cite{redmon2016yolo} and \textbf{SSD} \cite{liu2016ssd}, are designed for real-time applications, as they can process images quickly while still achieving competitive accuracy.
One-stage means that they predict bounding boxes and class probabilities directly from the input image in a single forward pass, skipping the need for region proposal generation before classification.
With this design they are able to achieve high inference speeds, making them suitable for applications like autonomous driving and surveillance, where real-time performance is crucial.

\subsection{Video Object Detection}
Compared to image-based object detection, video object detection resarch has received less attention, even though many real-world applications involve video data.
In most cases the video is treated as a sequence of individual frames, and image-based object detection models are applied independently to each frame.
The reason for this is the increased complexity of video data, which requires models to not only consider spatial information within each frame but also temporal information across frames.
Combining spatial and temporal information effectively is a challenging task, as it requires models to handle issues like motion blur, occlusions, and changes in object appearance over time.
However, if done successfully, leveraging temporal context can lead to significant improvements in detection accuracy and stability.
Since 2017 several works have proposed methods have been proposed to address these challenges.

Most notable works in video object detection include \textbf{FGFA} \cite{zhu2017fgfa}, which uses optical flow to aggregate features from multiple frames, and \textbf{MEGA} \cite{chen2018mega}, which introduces a memory module to store and retrieve temporal information.
Optical flow is a technique that estimates the motion of objects between consecutive frames, allowing the model to align features from different frames before aggregation.
Two also well known models are \textbf{TransVOD} \cite{zhu2021transvod} and \textbf{YOLOV} \cite{liu2023yolov}, which represent state of the art approaches to video object detection, leveraging transformer architectures and temporal feature fusion techniques, respectively.
A more in depth description of these two models is given in Chapter 2.

\subsection{Robustness in Computer Vision}
Robustness is as defined by Hendrycks and Dietterich \cite{hendrycks2019benchmarking} the ability of a model to maintain predictive performance under distribution shifts caused by common, naturally occurring image corruptions and perturbations.
Hendrycks and Dietterich were the first to investigate the robustness of image classification models under common corruptions and perturbations, introducing the benchmark suite \textbf{ImageNet-C} \cite{hendrycks2019benchmarking}.
They applied 15 different types of corruptions, such as Gaussian noise, motion blur, and brightness changes, to the ImageNet dataset and evaluated the performance degradation of various image classification models.
A more recent work by Liu et al. \cite{liu2024UAVRobustness} specifically focuses on the robustness of object detection models in UAV-based imagery.
They proposed the benchmark suite \textbf{UAV-C}, which consists of common corruptions and perturbations that frequently occur in UAV-based imagery, such as weather effects, motion blur, and illumination changes.

\section{Overview}
The thesis is structured as follows:

In Chapter 2, the video object detection models used in this thesis, TransVOD and YOLOV, are described in detail, including their architectures and mechanisms for leveraging temporal context.
After that the dataset used for evaluation on UAV-based imagery, Visdrone, is introduced in Chapter 3, along with the definition and implementation of common perturbations applied to the data for robustness evaluation.
To support reproducibility as well as describe the experimental setup, Chapter 4 outlines the training and evaluation procedures, including hyperparameter settings and hardware specifications.
Chapter 5 presents the results of the robustness evaluation, analyzing the impact of temporal context on detection performance under various perturbations.
Finally, Chapter 6 concludes the thesis with a summary of findings, discusses limitations, and suggests directions for future research in robust video object detection for UAV applications.

\chapter{Method}

\section{Model Overview}
For this thesis two video detector models are chosen, which represent different approaches to leverage temporal context in different ways.
On the one hand there is TransVOD \cite{zhu2021transvod}, which is a transformer based model that uses attention mechanisms to aggregate temporal information from multiple frames.
On the other hand YOLOV \cite{liu2023yolov}, which is a one-stage detector that extends the popular YOLO architecture to video data by incorporating temporal feature fusion techniques.
For both models the Swin base backbone \cite{swin2021transformer} has been chosen, to ensure a fair compairson as well as petrain weights being available for both models.

%Write better
Together these models provide a good basis for evaluation, as they represent different design philosophies and represent pro and cons in terms of temporal context utilization, computational efficiency and detection accuracy.
\section{Transvod}
Transvod \cite{zhu2021transvod} is a end to end video object detection model base on DETR \cite{carion2020endtoendobjectdetectiontransformers}.
End to end mean that no hand crafted features as well as no post processing is needed, everthing is learned by the model itself.
The models fist version was proposed in 2021 as one of the first transformer based video object detection models to streamline the detection pipeline and remove the need for hand crafted features.
By encoding not only spatial but also temporal information in their attention mechanism, the model shows strong performance on various video object detection benchmarks.
In the most well known video object detection benchmark, ImageNet VID \cite{russakovsky2015ImageNet} outperforms its single frame baseline by 3.6 mAP(\%) achieving 80.7 mAP(\%) on the validation set.
One year later an improved version of TransVOD was proposed, called TransVOD++ \cite{zhu2022transvod++}, which builds upon the original TransVOD architecture and introduces several enhancements to further improve detection performance.
The main goal of TransVOD++ is to address the heavy computation costs as well as increase the detection accuracy of its predecessor.
Next to architectural improvements, which i will describe in the following, a new backbone namely Swin-Base \cite{swin2021transformer} instead of ResNet-101 \cite{he2016ResNet} was used to further boost performance.
With these improvements TransVOD++ was the first model to achieve over 90 mAP(\%) on the ImageNet VID validation set, reaching 90.0 mAP(\%).
\subsubsection{Model design}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{graphics/transvod/model.png}
    \caption{TransVOD++ architecture overview (from \cite{zhu2022transvod++})}
    \label{fig:transvod++_architecture}
\end{figure}
TransVOD++ builds upon the deforamble DETR \cite{zhu2021deformabledetr} architecture, which itself improves the DETR \cite{carion2020endtoendobjectdetectiontransformers} model by introducing deformable attention modules to better handle multi-scale features and improve convergence speed.
For the backbone of the model the Swin-Base \cite{swin2021transformer} architecture is used, which is a hierarchical vision transformer that utilizes shifted windows for efficient self-attention computation.
To leverage temporal context of multible frames, TransVOD++ introduces two key components: Query and ROI fusion (QRF) and Hard Query Mining (HQM).

\paragraph{Query and ROI fusion (QRF)}
The goal of this module, as described in \cite{zhu2022transvod++}, is to reduce computational cost while still effectively leveraging temporal context.
QRF enables temporal encoding over object-level, RoI-refined feature embeddings instead of dense spatial feature maps. 
This is done by extracting RoI-aligned appearance features from predicted bounding boxes and fusing them into the corresponding transformer queries, allowing temporal aggregation to operate directly on object-centric representations.

\paragraph{Hard Query Mining (HQM)}
As the QRF module focuses on object-level features, HQM further reduces computational cost by retaining only the most informative object queries for temporal aggregation.
This is achieved by evaluating object queries from the current frame and all reference frames using a lightweight classification head and selecting only those with high confidence scores.
With this mechanism, redundant and low-confidence queries are discarded, allowing temporal fusion to operate on a compact and informative set of object queries.

As it can be seen in Figure~\ref{fig:transvod++_architecture}, each image is first processed by the deformable DETR backbone to extract spatial features.
After that the QRF module extracts RoI-aligned object features from predicted bounding boxes and fuses them into the corresponding object queries.
These features are then selected by the HQM module based on their confidence scores.
Finally, the selected object queries from the current frame and reference frames are aggregated using a temporal transformer, described in \ref{fig:transvod++_architecture} as Temporal Query Encoder (TQE) and Temporal
Deformable Transformer Encoder to produce the final detection results.


\section{YOLOV}
Based on the popular YOLO \cite{redmon2016yolo} architecture, to be more precise YOLOX \cite{yolox2021}, which are one-stage detectors known for their speed and efficiency, YOLOV \cite{liu2023yolov} extends this architecture to video data by incorporating temporal feature fusion techniques.
The paper that was published in 2023, was able to surpass previous state of the art video object detection models on the ImageNet VID \cite{russakovsky2015ImageNet} benchmark with a mAP(\%) of 85.5 on the validation set, while being still near real time capable with 22.7 FPS on a Nvidia TITAN RTX GPU.
YOLOV achieves this by introducing a temporal feature fusion module that aggregates features from multiple frames, allowing the model to leverage temporal context effectively.
Futhermore like TransVOD a improved version of YOLOV was proposed called YOLOV++ \cite{shi2024yolovpp}, which further enhances the temporal feature fusion mechanism and introduces additional optimizations to improve detection accuracy and efficiency.
The now improved YOLOV++ was able to achieve a new record mAP(\%) of 93.2 on the ImageNet VID validation set, while still being over 30 FPS on a Nvidia RTX 3090. 
However for this a special backbone names FocalNet-Large \cite{yang2022FocalModulation} is used.
Using the same Swin-Base \cite{swin2021transformer} backbone as for TransVOD++, YOLOV++ achieves a mAP(\%) of 90.7 on the ImageNet VID validation set, which is still significantly higher than TransVOD++ with 90.0 mAP(\%).
\subsubsection{Model design}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{graphics/yolov/model.png}
    \caption{YOLOV++ architecture overview (from \cite{shi2024yolovpp})}
    \label{fig:yolov++_architecture}
\end{figure}
The YOLOX architecture, which serves as the basis for YOLOV, improves upon previous YOLO versions by introducing several enhancements, such as an anchor-free design, a decoupled head which splits the classification and regression tasks, as well as OTA (Dynamic label assignment) to improve training efficiency and detection accuracy.
Based on this architecture, YOLOV++ adds the temporal context after the YOLOX processes each frame individually on the proposal-feature level.
To achieve this, YOLOV++ introduces the Feature Selection Module (FSM), which as the name suggests selects the most relevant features from the all frames, as well as the Feature Aggregation Module (FAM), which aggregates features from multiple frames to enhance the feature representation for object detection.

\paragraph{Feature Selection Module (FSM)}
Since one stage detectors like YOLOX generate a large number of proposals per frame, processing all proposals from multiple frames would be computationally infisable.
Therefore the FSM is introduced to select only the most relevant proposals based on there confidence for temporal aggregation, as well as adding NMS (Non Maximum Suppression) to remove redundant proposals.
This way the number of proposals goes from thousands per frame to only a few hundred, making temporal aggregation feasible.

\paragraph{Feature Aggregation Module (FAM)}
The FAM is responsible for aggregating the selected features from multiple frames. This is done by to parallel modules, one for classification features and one for regression features.
These modules use multi head attention to effectively combine information from different frames, however a homogenity issue is described in the paper, where standard attention tends to focus on similar features across frames, which could all be blurred, occluded or low quality.
To address this, they propose their key innovation, called Affinity Manner, where the similarity between proposals is weighted by their predicted confidence, so that temporally aggregated features are drawn preferentially from high-quality, reliable frames instead of uniformly similar but degraded ones.

All together the YOLOV++ architecture can be seen in Figure~\ref{fig:yolov++_architecture}, where each frame is first processed individually by the YOLOX backbone and neck to extract spatial features and generate proposals.
After that the FSM selects the most relevant proposals from all frames, which are then aggregated by the FAM using the Affinity Manner to produce enhanced features for final detection.
This way YOLOV++ effectively leverages temporal context while maintaining the efficiency and speed characteristic of one-stage detectors.

\chapter{Dataset and Metrics}

\section{Visdrone Dataset}
The Visdrone dataset \cite{zhu2021detection} is a large-scale dataset for different detection scenarios based on drone based imagery.
The images and videos were captured by various drone platforms in different urban and suburban areas of 14 different cities across China.
The objects are entitys of public street scenes, e.g., pedestrians, vehicles, bicycles, etc.
All together there are 10 different object categories.
It is curated by the \emph{AISKYEYE} research group from the \emph{Tianjin University in China}.
For this thesis VISDRONE2019-VID is used, to leverage the temporal context of the videos for the object detection task. 
All together the dataset contains 79 sequences with 33,366 frames, which are split 56 videos with 24,198 frames for training, 7 videos with 2,846 frames for validation and 16 videos with 6,322 frames for testing.
The dataset is chosen because of its large size and the challenging scenarios, e.g., different weather conditions, various altitudes and camera angles as well as high density of objects in the images.
Object sizes vary significantly, ranging from very small objects with only a few pixels to large objects covering a significant portion of the image.
This large difference in object sizes makes it also suitable to evaluate the performance of detection models under different perturbations and across different scales.

Since the dataset includes ignored regions, which are areas in the images where objects are not annotated due to beeing too crowded or too small, these regions are taken out of the evaluation to avoid penalizing the models for false positives in these areas.
This was done by blacking out these regions in the images before feeding them into the models for training and evaluation.
The method was chosen due to its simplicity and effectiveness. 
Other methods such as removing the annotations for these regions or masking them out during evaluation were considered, but blacking them out directly in the images was found to be the most straightforward approach.
Without this step models tend to produce a high number of false positives in these ignored regions, which would skew the evaluation results.
%show original and blacked out image
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/blackout/original.jpg}
        \caption{Original Image}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/blackout/blackout.jpg}
        \caption{Image with Ignored Regions Blacked Out}
    \end{subfigure}
    \caption{Example of an image from the Visdrone dataset with ignored regions blacked out for training and evaluation.}
    \label{fig:visdrone_ignored_regions}
\end{figure}




%exmaple images from the dataset
\begin{figure}[htbp]
    \centering

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{graphics/visdrone_example_1.jpg}
        \caption{Example 1}
    \end{subfigure}

    \par\medskip

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{graphics/visdrone_example_2.jpg}
        \caption{Example 2}
    \end{subfigure}

    \par\medskip

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{graphics/visdrone_example_3.jpg}
        \caption{Example 3}
    \end{subfigure}

    \caption{Example images from the Visdrone dataset.}
    \label{fig:visdrone_examples}
\end{figure}

\section{Perturbation}
A significant part of this thesis to evaluate the robustness of video object detection models under common perturbations in UAV-based imagery.
In this sentence two key concepts must be defined: what robustness means in the context of an object detection model, and which perturbations commonly occur in UAV-based imagery.

\subsection{Definition of Robustness}
A widely used definition of robustness was proposed by Hendrycks and Dietterich~\cite{hendrycks2019benchmarking}, who define robustness as a model’s ability to maintain predictive performance under distribution shifts caused by common, naturally occurring image corruptions and perturbations.
In the context of object detection, this means that a robust model should be able to accurately detect and localize objects even when the input images are affected by various types of noise, distortions, or other adverse conditions.
In the context of this thesis, the main metric to quantify robustness is the relative performance degradation of a model mean avarage precision (mAP) under different perturbations compared to its performance on clean data.
\subsection{Common Perturbations in UAV-based Imagery}
The paper by Hendrycks and Dietterich~\cite{hendrycks2019benchmarking} introduced a benchmark suite called ImageNet-C, which consists of 15 different types of common image corruptions applied to the ImageNet dataset.
For further insight a paper named \emph{Benchmarking the Robustness of UAV Tracking Against Common Corruptions} \cite{liu2024UAVRobustness} which was published in 2024, is used to identify perturbations that commonly occur in UAV-based imagery.
Based on these works, the following perturbations are considered in this thesis:
\begin{itemize}
    \item Gaussian Noise: Random noise following a Gaussian distribution is added to the image pixels, simulating sensor noise.
    \item Motion Blur: Simulates the effect of camera or object motion during exposure, resulting in blurred images.
    \item Defocus Blur: Simulates the effect of an out-of-focus lens, resulting in blurred images.
    \item Brightness Changes: Adjusts the overall brightness of the image, simulating different lighting conditions.
    \item Contrast Changes: Adjusts the contrast of the image, affecting the distinction between light and dark areas.
    \item Jpeg Compression: Simulates artifacts introduced by JPEG compression at various quality levels.
\end{itemize}
The simulation of weather conditions such as fog, rain, and snow is not considered in this thesis, as these perturbations require more complex rendering techniques.
For each perturbation type, multiple severity levels are defined to assess robustness across a range of adverse conditions; in this thesis, three levels are used: low, medium, and high.
\subsection{Implementation}
To apply the defined perturbations to the Visdrone dataset, a custom data augmentation pipeline is implemented into to the models data loader.
Based on evaluation input parameters, the data loader applies the specified perturbation with the desired severity level to each frame before it is fed into the model for inference.
For more in depth evaluation a probability parameter is added, which defines the likeliness each perturbation being applied to a frame.

\paragraph{Gaussian noise.}
We add i.i.d.\ Gaussian noise:
\begin{equation}
\tilde{I} = \mathrm{clip}\big(I + N,\ 0,\ 255\big), \qquad
N_{h,w,c} \sim \mathcal{N}\!\left(0,\ (\sigma \cdot 255)^2\right),
\end{equation}
where $\sigma$ is the noise standard deviation.

\paragraph{Defocus blur.}
We approximate defocus blur by convolving the image with a normalized disk (pillbox) kernel:
\begin{equation}
\tilde{I} = I * K_{\mathrm{disk}}, \qquad
K_{\mathrm{disk}}(u,v)=\frac{1}{Z}\,\mathbb{1}\!\left(u^2+v^2 \le r^2\right),
\end{equation}
where $K_{\mathrm{disk}}$ is a $k\times k$ kernel, $r=\lfloor k/2 \rfloor$, $Z=\sum_{u,v} \mathbb{1}(\cdot)$, and $*$ denotes 2D convolution.

\paragraph{Motion blur.}
We simulate linear motion blur by convolving with a sparse line kernel of size $k\times k$ oriented by an angle $\theta$ (in degrees, default $\theta=0$). The kernel is constructed by placing ones on the discrete line
\begin{equation}
v = \tan(\theta)\,u, \qquad u \in \left[-\left\lfloor k/2\right\rfloor,\ \left\lfloor k/2\right\rfloor\right],
\end{equation}
rasterized onto the kernel grid and normalized to sum to one, then $\tilde{I} = I * K_{\mathrm{motion}}$.

\paragraph{Brightness change.}
We apply a global multiplicative gain:
\begin{equation}
\tilde{I} = \mathrm{clip}\big(\alpha I,\ 0,\ 255\big),
\end{equation}
with $\alpha>0$.

\paragraph{Contrast change.}
We scale deviations from the per-channel mean:
\begin{equation}
\mu_c = \frac{1}{HW}\sum_{h,w} I_{h,w,c}, \qquad
\tilde{I}_{h,w,c} = \mathrm{clip}\big((I_{h,w,c}-\mu_c)\alpha + \mu_c,\ 0,\ 255\big),
\end{equation}
with contrast factor $\alpha$.

\paragraph{Pixelation.}
We downsample and upsample the image using a block factor $p$ (default $p=8$). Specifically, we resize $I$ to $(\lfloor W/p\rfloor, \lfloor H/p\rfloor)$ using bilinear interpolation, then resize back to $(W,H)$ using nearest-neighbor interpolation:
\begin{equation}
\tilde{I} = \mathrm{NN}\big(\mathrm{BL}(I; \lfloor W/p\rfloor,\lfloor H/p\rfloor);\ W,H\big),
\end{equation}
where $\mathrm{BL}$ denotes bilinear resize and $\mathrm{NN}$ denotes nearest-neighbor resize.

\paragraph{JPEG compression.}
We simulate compression artifacts by encoding and decoding the image using JPEG with quality parameter $q$:
\begin{equation}
\tilde{I} = \mathrm{JPEGdecode}\big(\mathrm{JPEGencode}(I; q)\big).
\end{equation}

The specific severity levels are defined as follows:


\begin{table}[htbp]
\centering
\caption{Perturbation presets and severity levels used in robustness evaluation.}
\rowcolors{2}{tablerow}{white}
\begin{tabular}{l c c c}
\toprule
\rowcolor{tableheader}
\textbf{Perturbation} & \textbf{Low} & \textbf{Medium} & \textbf{High} \\
\midrule
Gaussian noise
& $\sigma = 0.01$
& $\sigma = 0.05$
& $\sigma = 0.10$ \\
Defocus blur
& $k = 3$
& $k = 7$
& $k = 11$ \\
Motion blur
& $k = 3,\ \theta = 0^\circ$
& $k = 7,\ \theta = 0^\circ$
& $k = 15,\ \theta = 0^\circ$ \\
Brightness change
& $\alpha = 1.10$
& $\alpha = 1.25$
& $\alpha = 1.45$ \\
Contrast change
& $\alpha = 1.10$
& $\alpha = 1.25$
& $\alpha = 1.45$ \\
Pixelation
& $p = 2$
& $p = 4$
& $p = 6$ \\
JPEG compression
& $q = 85$
& $q = 55$
& $q = 25$ \\
\bottomrule
\end{tabular}
\end{table}

With this setup, all together 18 different perturbation configurations (6 perturbation types $\times$ 3 severity levels) can be evaluated to assess the robustness of video object detection models in UAV-based imagery.

\begin{figure}[htbp]
\centering
\caption{Example images illustrating perturbation severities (Low, Medium, High).}
\label{fig:perturbation_examples}

\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{l c c c}
\toprule
\textbf{Perturbation} & \textbf{Low} & \textbf{Medium} & \textbf{High} \\
\midrule

\raisebox{5\height}{Gaussian noise}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/gaussian_noise_low.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/gaussian_noise_mid.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/gaussian_noise_high.jpg} \\

\raisebox{5\height}{Defocus blur}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/defocus_blur_low.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/defocus_blur_mid.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/defocus_blur_high.jpg} \\

\raisebox{5\height}{Motion blur}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/motion_blur_low.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/motion_blur_mid.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/motion_blur_high.jpg} \\

\raisebox{5\height}{Brightness change}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/brightness_change_low.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/brightness_change_mid.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/brightness_change_high.jpg} \\

\raisebox{5\height}{Contrast change}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/contrast_change_low.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/contrast_change_mid.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/contrast_change_high.jpg} \\

\raisebox{5\height}{Pixelation}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/pixelation_low.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/pixelation_mid.jpg}
& \includegraphics[width=0.20\linewidth]{graphics/visdrone_perturbated_examples/pixelation_high.jpg} \\

\bottomrule
\end{tabular}
\end{figure}


\section{Evaluation Metrics}
For robustness evaluation next to standard object detection metrics such as mean average precision (mAP) and mean average recall (mAR), a proper metric is needed, that quantifies the performance degradation of a model under different perturbations relative to its performance on clean data.
Derived from the papers \emph{Benchmarking the Robustness of UAV Tracking Against Common Corruptions} \cite{liu2024UAVRobustness} mCE (mean corruption error) which was based on the top-1 error rate, a new metric named \emph{mean Corruption Average Precision} (mCAP) and \emph{relative mean Corruption Average Precision} (rmCAP) is proposed.
Parallel to that \emph{mean Corruption Average Recall} (mCAR) and \emph{relative mean Corruption Average Recall} (rmCAR) is defined.

\paragraph{Mean Corruption Average Precision (mCAP)} is defined as: 

\begin{equation}
\mathrm{mCAP}
=
\frac{1}{|C|}
\sum_{c \in C}
\frac{1}{S}
\sum_{s=1}^{S}
\mathrm{mAP}_{s,c},
\end{equation}

where $C$ is the set of all perturbation configurations (i.e., perturbation types and severity levels), $S$ is the number of severity levels, and $\mathrm{mAP}_{s,c}$ is the mean average precision of the model under perturbation configuration $c$ at severity level $s$.

\paragraph{Relative mean Corruption Average Precision (rmCAP)} is defined as:
\begin{equation}
\mathrm{rmCAP}
=
\frac{1}{|C|}
\sum_{c \in C}
\frac{1}{S}
\sum_{s=1}^{S}
\frac{\mathrm{mAP}_{s,c}}{\mathrm{mAP}_{\mathrm{clean}}},
\end{equation}

where $\mathrm{mAP}_{\mathrm{clean}}$ is the mean average precision of the model on clean, unperturbed data.

\paragraph{Probability-based mean Corruption Average Precision (p-mCAP).}
In addition to corruption-type robustness, we evaluate robustness with respect to the
probability of perturbation occurrence.
Let $P$ denote the set of perturbation probabilities applied independently per frame.
We define probability-based mean Corruption Average Precision (p-mCAP) as
\begin{equation}
\mathrm{p\text{-}mCAP}
=
\frac{1}{|P|}
\sum_{p \in P}
\frac{1}{S}
\sum_{s=1}^{S}
\mathrm{mAP}_{p,s},
\end{equation}
where $\mathrm{mAP}_{p,s}$ denotes the detection performance when each frame is perturbed
with probability $p$ at severity level $s$.

\paragraph{Relative probability-based mean Corruption Average Precision (rp-mCAP).}
Analogous to rmCAP, we define a relative probability-based robustness metric by normalizing
p-mCAP with respect to a baseline detector:
\begin{equation}
\mathrm{rp\text{-}mCAP}
=
\frac{1}{|P|}
\sum_{p \in P}
\frac{
\sum_{s=1}^{S} \mathrm{mAP}^{f}_{p,s}
}{
\sum_{s=1}^{S} \mathrm{mAP}^{\text{baseline}}_{p,s}
}.
\end{equation}



\chapter{Experiments Setup}

\section{Hardware and Software Environment}
The training as well as evaluation of the models was done on eather Nvidia RTX 3060 Ti or Nvidia RTX 3090ti GPUs.
This was due due to the fact that CUDA 11.3 was required by the TransVOD implementation, which is not supported by the newer GPUs such as the RTX 40 series.
Both models were implemented in PyTorch \cite{paszke2019pytorch} and trained using the AdamW \cite{loshchilov2017decoupled} optimizer.

\section{Reference Frame Sampling Strategy} 
Since both models leverage temporal context from multiple frames, a proper reference frame sampling strategy is needed to select the frames that will be used as input to the models.
As written in the original papers as well as implemented int the provided code repositories, TransVOD++ and YOLOV++ use inherently different sampling strategies.

\subsection{Transvod Sampling}
Following the sampling strategy of TransVOD++, each video is divided into 16 temporal intervals. 
For a given target frame, reference frames are selected by sampling one frame from each interval. 
When the number of reference frames $N\geq8$, sampling is performed only on one side of the target frame; when $N\geq8$ reference frames are sampled from both the past and future relative to the target frame.
As a result, the selected reference frames are approximately evenly spaced in time around the target frame.

However, as will be discussed later, this global sampling strategy does not achieve reasonable performance on the VisDrone dataset.
To address this issue, we adopt a modified, more local sampling strategy in which reference frames are selected using a fixed temporal offset relative to the target frame.
Specifically, we test reference frames sampling with a temporal offset of 1 frame and 8 frames.
Overall, one single-frame baseline and twelve video-based TransVOD++ configurations were evaluated, combining three reference-frame sampling strategies with four different numbers of reference frames.
\subsection{YOLOV Sampling}
In the YOLOV++ paper, it is mentioned that test have resulted in better performance when using random sampling of reference frames in the whole video clip, called global sampling.
Therefore, for YOLOV++ the original global sampling strategy is used, where reference frames are sampled from the entire video clip.
To test the impact of local sampling on YOLOV++, an additional configuration with local sampling is evaluated, where reference frames are selected using a fixed temporal offset relative to the target frame with stride 10.
Therefore, a total of seven YOLOV++ configurations were evaluated, consisting of one single-frame baseline and six video-based variants formed by combining two reference-frame sampling strategies with three different reference-frame counts.


\section{Input Resolution}
Since the VisDrone dataset contains a large proportion of small objects, which are particularly challenging for object detectors, the input resolution must be chosen carefully. Higher resolutions preserve fine details and improve small-object detectability, but they also increase memory consumption and thus limit the feasible batch size on the available GPUs. 
Consequently, an input resolution of 960x544 pixels was chosen as a compromise between detection performance and memory efficiency, as well as keeping the original aspect ratio of the images.

\section{Training Protocol}
Both models are trained under an identical training protocol to ensure a fair comparison. Differences between the two models are limited to architectural components and are detailed separately.
Since both models already provided pretrained weights on the ImageNet VID \cite{russakovsky2015ImageNet} dataset with a Swin-Base \cite{swin2021transformer} backbone, these weights were used to finetune the single frame version of the models on the Visdrone2019-VID \cite{zhu2021detection} training set.
\begin{table}[htbp]
\centering
\caption{Fine-tuning hyperparameters for the single-frame detectors.}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{4pt}

\begin{tabular}{p{0.22\textwidth} p{0.35\textwidth} p{0.35\textwidth}}
\toprule
\rowcolor{tableheader}
\textbf{Hyperparameter} & \multicolumn{1}{c}{\textbf{YOLOV++}} & \multicolumn{1}{c}{\textbf{TransVOD++}} \\
\midrule
Batch size     & \multicolumn{1}{c}{$8$} & \multicolumn{1}{c}{$8$} \\
Precision      & \multicolumn{1}{c}{FP16} & \multicolumn{1}{c}{FP16} \\
Learning rate  & \multicolumn{1}{c}{$0.01/64 \approx 1.56 \cdot 10^{-4}$} & \multicolumn{1}{c}{\makecell[c]{$10^{-4}$ and $10^{-5}$ for backbone,\\ with lr drops after 3 and 5 epochs}} \\
Weight decay   & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} \\
Data augmentation
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
    \item Multi-scale resize: original size $\pm 64$ px
    \item Color jitter
    \item Geometric transforms: rotation $\pm 5^\circ$, translation $0.1$, shear $2^\circ$
\end{itemize}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
    \item Multi-scale resize: original size $\pm 64$ px
    \item Color jitter
\end{itemize}
\end{minipage}
\\
\bottomrule
\end{tabular}
\end{table}

After finetuning the single frame models, the video versions were trained by loading the finetuned single frame weights and training the temporal context modules while keeping the backbone and single frame detection head frozen.
For TransVOD++ the number of reference frames was used namely 1, 3, 7 and 15 reference frames.
However due to memory constraints only up to 7 reference frames could be used for YOLOV++ since the number of reference frame directly corresponds to the batch size.
This means that for 15 reference frames a batch size of 16 would be required, which surpasses the available GPU memory of 24GB of the NVIDIA RTX 3090ti.

For training the video models, different hyperparameters were used compared to the single frame models, which are summarized in the following table.
\begin{table}[htbp]
\centering
\caption{Fine-tuning hyperparameters for the video detectors.}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{4pt}

\begin{tabular}{p{0.22\textwidth} p{0.35\textwidth} p{0.35\textwidth}}
\toprule
\rowcolor{tableheader}
\textbf{Hyperparameter} & \multicolumn{1}{c}{\textbf{YOLOV++}} & \multicolumn{1}{c}{\textbf{TransVOD++}} \\
\midrule
Batch size     & \multicolumn{1}{c}{number of reference frames} & \multicolumn{1}{c}{$1$} \\
Precision      & \multicolumn{1}{c}{FP16} & \multicolumn{1}{c}{FP16} \\
Learning rate  & \multicolumn{1}{c}{$10^{-5}$} & \multicolumn{1}{c}{\makecell[c]{$2 \cdot 10^{-6}$}} \\
Weight decay   & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} \\
Data augmentation
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
\end{itemize}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
\end{itemize}
\end{minipage}
\\
\bottomrule
\end{tabular}
\end{table}

All together the single frame as well as 3 different video models for YOLOV++ and 4 different video models for TransVOD++ were trained and evaluated.

For all the evaluations random seeds were fixed to 42 to ensure reproducibility of the results, except for random sample selection of reference frames during training, where different random seeds were used to increase the diversity of training samples.
To see if this had an impact on the results, 5 different training runs were done for the YOLOV++ model with 7 reference frames, which showed almost no variation in mAP(\%) of less than 0.3\% between the runs.

For evaluation the models were first evaluated on the clean Visdrone2019-VID \cite{zhu2021detection} validation set to obtain the baseline mAP(\%) and mAR(\%) values.
Subsequently the models were evaluated on the perturbed versions of the validation set for each perturbation type and severity level as described in the \emph{Perturbation} chapter.

To evaluate whether temporal context enables models to maintain detection consistency when individual frames are perturbed, we conducted additional experiments using a probabilistic perturbation strategy. Specifically, we randomly selected 10\% and 20\% of frames within each video sequence and applied a diverse set of perturbations at high severity levels. This approach allows us to assess whether models can leverage information from adjacent unperturbed frames to mitigate the impact of corruptions on isolated frames.
\chapter{Empirical Results}
For the results alltogether 20 differnt models were evaluated, consisting of one single frame baseline and six video-based variants of YOLOV++ as well as one single frame baseline and twelve video-based variants of TransVOD++.
All models were first evaluated on the clean Visdrone2019-VID \cite{zhu2021detection} validation set to obtain baseline performance metrics, followed by evaluations on perturbed versions of the validation set for each defined perturbation type and severity level.
These results are then combined to compute the proposed robustness metrics mCAP, rmCAP, mCAR, and rmCAR.
As mentioned before, to further investigate the benefits of temporal context in handling sporadic perturbations, additional evaluations were performed using a probabilistic perturbation strategy where 10\% and 20\% of frames in each video sequence were randomly selected for high-severity perturbations.
\section{Evaluation on Clean Data}

To form a basis for the robustness evaluation, the models were first evaluated on the clean Visdrone2019-VID \cite{zhu2021detection} validation set.
The results are summarized in Table~\ref{tab:clean_results}.
\begin{table}[htbp]
\centering
\caption{Detection performance of all evaluated video object detection models on the VisDrone2019-VID validation set. (Best results in \textbf{bold} and worst results in \underline{underline} per model.)}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{6pt}

\begin{tabular}{p{0.50\textwidth} c c}
\toprule
\rowcolor{tableheader}
\textbf{Model Configuration} & \textbf{mAP$_{50:95}$ (\%)} & \textbf{mAR$_{50:95}$ (\%)} \\
\midrule
\multicolumn{3}{l}{\textbf{YOLOV++}} \\
Single-frame baseline                 & \underline{17.2} & \underline{28.3} \\
Sampling random, 1 reference frame         & 17.5 & 29.2 \\
Sampling random, 3 reference frames        & 17.8 & 29.7 \\
Sampling random, 7 reference frames        & \textbf{18.0} & \textbf{30.0} \\
Sampling local, 1 reference frame         & 17.3 & 29.0 \\
Sampling local, 3 reference frames        & 17.7 & 29.5 \\
Sampling local, 7 reference frames        & 17.8 & \textbf{30.0} \\
\midrule
\multicolumn{3}{l}{\textbf{TransVOD++}} \\
Single-frame baseline                 & 19.4 & 34.8 \\
Sampling stride 1, 1 reference frame         & 19.6 & 35.8 \\
Sampling stride 1, 3 reference frames        & 20.0 & 36.4 \\
Sampling stride 1, 7 reference frames        & \textbf{20.4} & \textbf{36.3} \\
Sampling stride 1, 15 reference frames       & 19.8 & 35.2 \\
Sampling stride 8, 1 reference frame         & 19.3 & 35.2 \\
Sampling stride 8, 3 reference frames        & 19.7 & 35.6 \\
Sampling stride 8, 7 reference frames        & 19.4 & 34.6 \\
Sampling stride 8, 15 reference frames       & 18.2 & 32.8 \\
Sampling global, 1 reference frame         & 19.5 & 35.1 \\
Sampling global, 3 reference frames        & 19.3 & 34.8 \\
Sampling global, 7 reference frames        & 18.9 & 33.3 \\
Sampling global, 15 reference frames       & \underline{17.2} & \underline{31.0} \\
\bottomrule
\end{tabular}
\label{tab:clean_results}
\end{table}

As can it can be seen in Table~\ref{tab:clean_results}, the single frame baseline of TransVOD++ outperforms the single frame baseline of YOLOV++ by a significant margin of 2.2\% mAP and 6.5\% mAR.
When looking at the video-based configurations, both models show improvements over their respective single frame baselines.

For YOLOV++, the best performance is achieved using random sampling of reference frames, with 7 reference frames.
This configuration yields the highest mAP of 18.0\% and mAR of 30.0\%, therefore an increase of 0.8\% mAP and 1.7\% mAR or 4.7\% and 6.0\% relative improvement compared to the single frame baseline.
Alligned with the findings in the YOLOV++ paper, random sampling of reference frames outperforms local sampling in all configurations, however the performance gap is relatively small with only 0.2\% mAP and 0.0-0.5\% mAR difference between the two sampling strategies.

For TransVOD++, the best performance is achieved using a local sampling strategy with a temporal stride of 1 frame and 7 reference frames.
This configuration yields the highest mAP of 20.4\% and mAR of 36.3\%, therefore an increase of 1.0\% mAP and 1.5\% mAR or 5.2\% and 4.3\% relative improvement compared to the single frame baseline.
However, it is important to note that in contrast to the orginal TransVOD++ paper, the global sampling strategy results in the worst performance on the Visdrone dataset.
It even performs worse than the single frame baseline when using 15 reference frames, which indicates that this sampling strategy is not suitable for the Visdrone dataset.

To get a better insight into the impact of the sampling strategy, local sampling with a temporal stride of 8 frames was also evaluated.
This configuration shows a similar trend as global sampling, where the performance degrades with an increasing number of reference frames.
This further supports the hypothesis that sampling frames that are temporally distant from the target frame is not beneficial for the Visdrone dataset, likely due to the fast motion and rapid scene changes in UAV-based videos.

Since the global sampling strategy achieves the worst performance with a significant margin, it will not be considered for the robustness evaluation in the following chapter.
Therefore we we have a total of 9 instead of 13 different TransVOD++ configurations for robustness evaluation.



\section{Evaluation on Perturbed Data}
For robustness evaluation, the models were evaluated on the perturbed versions of the Visdrone2019-VID validation set for each defined perturbation type and severity level independently.
Every model was evaluated on all 18 perturbation configurations, and the results were aggregated to compute the proposed robustness metrics mCAP, rmCAP, mCAR, and rmCAR.
The combined model results on perturbed data are summarized in Table~\ref{tab:perturbated_results}.
For more detailed results on each perturbation type and severity level, please refer to the Appendix.

The results are summarized in Table~\ref{tab:clean_perturbated}.
\begin{table}[H]
\centering
\caption{Detection performance of all evaluated video object detection models on the VisDrone2019-VID validation set under perturbations. (Best results in \textbf{bold} and worst results in \underline{underline} per model.)}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{4pt}

\begin{tabular}{p{0.40\textwidth} c c c c}
\toprule
\rowcolor{tableheader}
\textbf{Model Configuration} & \textbf{mCAP$_{50:95}$ (\%)} & \textbf{mCAR$_{50:95}$ (\%)} & \textbf{rmCAP} & \textbf{rmCAR} \\
\midrule
\multicolumn{5}{l}{\textbf{YOLOV++}} \\
Single-frame baseline                 & \underline{11.3} & \underline{20.1} & 0.66 & \underline{0.71} \\
Sampling local, 1 reference frame     & 11.4 & 21.7 & 0.66 & 0.75 \\
Sampling local, 3 reference frames    & 11.5 & 21.8 & 0.65 & 0.74 \\
Sampling local, 7 reference frames    & 11.4 & 21.8 & \underline{0.64} & 0.73 \\
Sampling random, 1 reference frame    & 11.7 & 22.1 & 0.67 & 0.76 \\
Sampling random, 3 reference frames   & 11.9 & 22.6 & 0.67 & 0.76 \\
Sampling random, 7 reference frames   & \textbf{12.0} & \textbf{23.0} & \textbf{0.67} & \textbf{0.77} \\
\midrule
\multicolumn{5}{l}{\textbf{TransVOD++}} \\
Sampling stride 1, 1 reference frame & 15.7 & 28.6 & 0.84 & 0.84 \\
Sampling stride 1, 3 reference frames & 15.9 & 29.0 & 0.84 & 0.84 \\
Sampling stride 1, 7 reference frames & 16.1 & 29.2 & 0.85 & 0.85 \\
Sampling stride 1, 15 reference frames & 15.6 & 28.6 & 0.85 & 0.86 \\
Sampling stride 8, 1 reference frame & 15.5 & 28.2 & 0.85 & 0.84 \\
Sampling stride 8, 3 reference frames & 15.5 & 28.3 & 0.84 & 0.84 \\
Sampling stride 8, 8 reference frames & 15.3 & 27.9 & 0.83 & 0.85 \\
Sampling stride 8, 15 reference frames & 14.3 & 26.2 & 0.83 & 0.84 \\
\bottomrule
\end{tabular}
\label{tab:perturbated_results}
\end{table}

On perturbed data, TransVOD++ significantly outperforms YOLOV++ in terms of mCAP and mCAR across all configurations, as seen in Table~\ref{tab:perturbated_results}.
This performance gap can be noticed on every model configuration, with TransVOD++ achieving mCAP values ranging from 17.0\% to 19.1\%, while YOLOV++ achieves mCAP values between 11.3\% and 12.0\%.
Looking at the relative metrics rmCAP and rmCAR, TransVOD++ also demonstrates superior robustness to perturbations, on the other hand a signicant releative performance drop can be observed for YOLOV++.

The perturbation results for YOLOV++ in Table~\ref{tab:perturbated_results} show that the video-based configurations outperform the single-frame baseline in terms of mCAP and mCAR.
This phenomenon is consistent with the clean data results \ref{tab:clean_results}, where video-based models also showed improved performance.
Looking at the the realtive metrics rmCAP and rmCAR, no clear trend can be observed between model configurations. 
This indicates that the number of reference frames and sampling strategy have little impact on the relative robustness of YOLOV++ to perturbations.
In general the YOLOV++ models show a signicant relative performance drop when evaluated on perturbed data, with rmCAP values ranging from 0.64 to 0.67 and rmCAR values between 0.71 and 0.77.

For TransVOD++, a similar trend as in the clean data results can be observed in Table~\ref{tab:perturbated_results}, where the models gain from the first few reference frames but performance degrades when using too many reference frames.
Forthermore the local sampling stagegy with a temporal stride of 1 frame outperforms the stride of 8 frames strategy, with the best performance achieved using 7 reference frames.
However an important observation is that the relative robustness of all TransVOD++ configurations is shown a very strong robustness to perturbations, with rmCAP values ranging from 0.83 to 0.86 and rmCAR values between 0.84 and 0.86.
As with YOLOV++ no clear trend can be observed between model configurations, indicating that the number of reference frames and sampling strategy have little impact on the relative robustness of TransVOD++ to perturbations.

\subsection{Perturbations and Reference Frames}
Opposite of the initial hypothesis, increasing the number of reference frames does not necessarily lead to improved robustness against perturbations.
This trend is can be observed in both YOLOV++ and TransVOD++ models, while video models generally outperform their single-frame baselines, relative to their clean data performance, the robustness does not improve with more reference frames.
This indicates that simply adding more temporal context is not sufficient to enhance robustness against perturbations.
Therefore if all frames are perturbated, the models are not able to leverage information from multibiple perturbated frames to improve detection performance.

\section{Deeper insight into Perturbations}
For the insight on a perturbation level, 3 model configurations were selected for each YOLOV++ and TransVOD++. These configurations represent the single-frame baseline, as well the best performing temporal window size of each sampling strategy.
The key results are visualized in radar plots in Figure~\ref{fig:perturbation_analysis}. Each plot illustrates the mAP performance across different perturbation types and severity levels, providing a clear comparison of how each model configuration handles various corruptions.
\begin{figure}[htbp]
\centering
\caption{mAP performance across perturbation types and severity levels.}
\label{fig:perturbation_analysis}

\begin{tabular}{cc}
\includegraphics[width=0.45\linewidth]{graphics/plots/perturbation/radar_YOLOV_Single-frame_baseline.pdf} &
\includegraphics[width=0.45\linewidth]{graphics/plots/perturbation/radar_TransVOD_Single-frame_baseline.pdf} \\
\includegraphics[width=0.45\linewidth]{graphics/plots/perturbation/radar_YOLOV_Local_int=10_3_ref.pdf} &
\includegraphics[width=0.45\linewidth]{graphics/plots/perturbation/radar_TransVOD_Stride_1_7_ref.pdf} \\
\includegraphics[width=0.45\linewidth]{graphics/plots/perturbation/radar_YOLOV_Random_int=10_7_ref.pdf} &
\includegraphics[width=0.45\linewidth]{graphics/plots/perturbation/radar_TransVOD_Stride_8_3_ref.pdf} \\
\end{tabular}
\end{figure}
As already observed in the aggregated results, TransVOD++ consistently outperforms YOLOV++.
In general both models show a similar trend in handling different perturbation types.
However, looking at \textbf{pixelation} and \textbf{motion blur}, TransVOD++ shows a significantly better robustness compared to YOLOV++, especially at higher severity levels. 
For \textbf{brightness} and \textbf{contrast changes}, both models exhibit relatively stable performance where virtually no performance drop can be observed even at high severity levels.
When evaluating \textbf{defocus blur}, \textbf{gaussian noise} and \textbf{JPEG Compression}, both models experience a steady performance decline as severity increases, but TransVOD++ maintains a higher mAP across all severity levels.

As seen in the overall results, increasing the number of reference frames does not consistently enhance robustness across all perturbation types.
This strengthens the observation that simply adding more temporal context is not sufficient to improve robustness against perturbations, especially when all frames are affected.






\dots

\section{Single Frame Perturbation Results}
\small
\begin{table}[H]
\centering
\caption{Detection performance under probabilistic perturbations (10\% and 20\% of frames perturbed).}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{3pt}

\begin{tabular}{p{0.33\textwidth} c c c c c}
\toprule
\rowcolor{tableheader}
\textbf{Model Config.} & \textbf{Clean} & \textbf{mAP(10\%)} & \textbf{mAP(20\%)} & \textbf{Drop(10\%)} & \textbf{Drop(20\%)} \\
\midrule
\multicolumn{6}{l}{\textbf{YOLOV++}} \\
Single-frame & 17.2 & 16.4 & 15.5 & 4.7\% & 9.9\% \\
Random, 1 ref & 17.5 & 16.7 & 15.9 & 4.6\% & 9.1\% \\
Random, 3 ref & 17.8 & 17.1 & 16.4 & 3.9\% & 7.9\% \\
Random, 7 ref & 18.0 & 17.5 & 16.8 & 2.8\% & 6.7\% \\
Local, 1 ref & 17.3 & 16.6 & 15.7 & 4.0\% & 9.2\% \\
Local, 3 ref & 17.7 & 17.0 & 16.2 & 3.9\% & 8.5\% \\
Local, 7 ref & 17.8 & 17.3 & 16.7 & 2.8\% & 6.2\% \\
\midrule
\multicolumn{6}{l}{\textbf{TransVOD++}} \\
Single-frame & 19.4 & 18.8 & 18.2 & 3.1\% & 6.2\% \\
Stride 1, 1 ref & 19.6 & 19.1 & 18.5 & 2.6\% & 5.6\% \\
Stride 1, 3 ref & 20.0 & 19.5 & 19.0 & 2.5\% & 5.0\% \\
Stride 1, 7 ref & 20.4 & 20.0 & 19.5 & 2.0\% & 4.4\% \\
Stride 1, 15 ref & 19.8 & 19.4 & 18.9 & 2.0\% & 4.5\% \\
Stride 8, 1 ref & 19.3 & 18.8 & 18.3 & 2.6\% & 5.2\% \\
Stride 8, 3 ref & 19.7 & 19.3 & 18.8 & 2.0\% & 4.6\% \\
Stride 8, 7 ref & 19.4 & 19.1 & 18.7 & 1.5\% & 3.6\% \\
Stride 8, 15 ref & 18.2 & 18.0 & 17.7 & 1.1\% & 2.7\% \\
\bottomrule
\end{tabular}
\label{tab:single_frame_perturbations}
\end{table}
In contrast to the previous evaluations where all frames in a video sequence were perturbed, additional experiments were conducted using a probabilistic perturbation strategy.
This approach involved randomly selecting 10\% and 20\% of frames within each video sequence and applying a single perturbation at high severity level.
Therefore while single frame models have no way of recovering from the perturbation, video models can leverage information from adjacent unperturbed frames to mitigate the impact of corruptions on isolated frames.
This way we can evaluate wheather temporal context enables models to maintain detection consistency when individual frames are perturbed.
The results of these experiments are summarized in Table~\ref{tab:single_frame_perturbations}.
When looking at the results, all models as expected show a performance drop going from clean to perturbated data.
However in this experiment the video-based models show a smaller performance drop compared to their single-frame baselines, indicating that temporal context helps mitigate the impact of sporadic perturbations.
For example for YOLOV++, the single-frame baseline experiences a performance drop of 4.7\% and 9.9\% for 10\% and 20\% perturbed frames respectively, while the video-based model with random sampling and 7 reference frames only shows a drop of 2.8\% and 6.7\%.
This show a relative reduction in performance drop of 40.4\% and 32.3\% respectively.
A similar trend can be observed for TransVOD++, where the single-frame baseline experiences a performance drop of 3.1\% and 6.2\% for 10\% and 20\% perturbed frames respectively, while the video-based model with stride 1 sampling and 7 reference frames only shows a drop of 2.0\% and 4.4\%.
Also here a relative reduction in performance drop of 35.5\% and 29.0\% respectively can be observed.
This clearly shows that temporal context enables video object detection models to better handle sporadic perturbations by leveraging information from adjacent unperturbed frames.
For better visualization of the results, the performance drop values are also illustrated in the bar chart in Figure~\ref{fig:single_frame_perturbations}.



\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=5pt,
    width=0.99\linewidth,
    height=0.3\textheight,
    ylabel={Performance Drop (\%)},
    ymin=0, ymax=11,
    xlabel={Configuration},
    symbolic x coords={
        Y_Single,Y_R1,Y_R3,Y_R7,Y_L1,Y_L3,Y_L7,gap,
        T_Single,T_S1_1,T_S1_3,T_S1_7,T_S1_15,T_S8_1,T_S8_3,T_S8_7,T_S8_15
    },
    xtick={
        Y_Single,Y_R1,Y_R3,Y_R7,Y_L1,Y_L3,Y_L7,
        T_Single,T_S1_1,T_S1_3,T_S1_7,T_S1_15,T_S8_1,T_S8_3,T_S8_7,T_S8_15
    },
    xticklabels={
        Single, Random 1, Random 3, Random 7, Local 1, Local 3, Local 7,
        Single, Stride1 1, Stride1 3, Stride1 7, Stride1 15, Stride8 1, Stride8 3, Stride8 7, Stride8 15
    },
    xticklabel style={rotate=35, anchor=east},
    ymajorgrids,
    grid style={opacity=0.25},
    legend style={
        draw=none,
        font=\scriptsize,
        at={(0.5,-0.30)},
        anchor=north,
        legend columns=4
    },
    % Increase margins to prevent label clipping
    enlarge x limits=0.20,
]

% ================= YOLOV++ (BACK: 20%) =================
\addplot[
    ybar,
    fill=blue!25,
    draw=none,
    fill opacity=0.60,
    bar shift=3pt,  % cluster left + back offset
] coordinates {
    (Y_Single,9.9) (Y_R1,9.1) (Y_R3,7.9) (Y_R7,6.7) (Y_L1,9.2) (Y_L3,8.5) (Y_L7,6.2)
};
\addlegendentry{YOLOV++ 20\%}

% ================= YOLOV++ (FRONT: 10%) =================
\addplot[
    ybar,
    fill=blue!75,
    draw=none,
    fill opacity=1.00,
    bar shift=-1pt,  % cluster left + front offset
] coordinates {
    (Y_Single,4.7) (Y_R1,4.6) (Y_R3,3.9) (Y_R7,2.8) (Y_L1,4.0) (Y_L3,3.9) (Y_L7,2.8)
};
\addlegendentry{YOLOV++ 10\%}

% ================= TransVOD++ (BACK: 20%) =================
\addplot[
    ybar,
    fill=orange!35,
    draw=none,
    fill opacity=0.60,
    bar shift=3pt,  % cluster right + back offset
] coordinates {
    (T_Single,6.2)
    (T_S1_1,5.6) (T_S1_3,5.0) (T_S1_7,4.4) (T_S1_15,4.5)
    (T_S8_1,5.2) (T_S8_3,4.6) (T_S8_7,3.6) (T_S8_15,2.7)
};
\addlegendentry{TransVOD++ 20\%}

% ================= TransVOD++ (FRONT: 10%) =================
\addplot[
    ybar,
    fill=orange!85,
    draw=none,
    fill opacity=1.00,
    bar shift=-1pt,  % cluster right + front offset
] coordinates {
    (T_Single,3.1)
    (T_S1_1,2.6) (T_S1_3,2.5) (T_S1_7,2.0) (T_S1_15,2.0)
    (T_S8_1,2.6) (T_S8_3,2.0) (T_S8_7,1.5) (T_S8_15,1.1)
};
\addlegendentry{TransVOD++ 10\%}

\end{axis}
\end{tikzpicture}

\caption{Performance drop under probabilistic perturbations.
For each configuration, bars are grouped by model (YOLOV++ left, TransVOD++ right).
Within each model group: 20\% (back, lighter) and 10\% (front, darker).}
\label{fig:drop_both_models_oneplot}
\end{figure}












% \small
% \begin{table}[htbp]
% \centering
% \caption{Detection performance under probabilistic perturbations (10\% and 20\% of frames perturbed).}
% \rowcolors{2}{tablerow}{white}

% \small
% \setlength{\tabcolsep}{3pt}

% \begin{tabular}{p{0.33\textwidth} c c c c c}
% \toprule
% \rowcolor{tableheader}
% \textbf{Model Config.} & \textbf{Clean} & \textbf{mAR(10\%)} & \textbf{mAR(20\%)} & \textbf{Drop(10\%)} & \textbf{Drop(20\%)} \\
% \midrule
% \multicolumn{6}{l}{\textbf{YOLOV++}} \\
% Single-frame & 28.3 & 27.1 & 25.9 & 4.2\% & 8.5\% \\
% Random, 1 ref & 29.2 & 28.1 & 26.8 & 3.8\% & 8.2\% \\
% Random, 3 ref & 29.7 & 28.7 & 27.7 & 3.4\% & 6.7\% \\
% Random, 7 ref & 30.0 & 29.3 & 28.4 & 2.3\% & 5.3\% \\
% Local (int=10), 1 ref & 29.0 & 27.9 & 26.7 & 3.8\% & 7.9\% \\
% Local (int=10), 3 ref & 29.5 & 28.5 & 27.4 & 3.4\% & 7.1\% \\
% Local (int=10), 7 ref & 30.0 & 29.2 & 28.4 & 2.7\% & 5.3\% \\
% \midrule
% \multicolumn{6}{l}{\textbf{TransVOD++}} \\
% Single-frame & 34.8 & 33.9 & 32.9 & 2.6\% & 5.5\% \\
% Local (int=1), 1 ref & 35.8 & 34.8 & 33.7 & 2.8\% & 5.9\% \\
% Local (int=1), 3 ref & 36.4 & 35.5 & 34.5 & 2.5\% & 5.2\% \\
% Local (int=1), 7 ref & 36.3 & 35.6 & 34.8 & 1.9\% & 4.1\% \\
% Local (int=1), 15 ref & 35.2 & 34.5 & 33.8 & 2.0\% & 4.0\% \\
% Local (int=8), 1 ref & 35.2 & 34.3 & 33.3 & 2.6\% & 5.4\% \\
% Local (int=8), 3 ref & 35.6 & 34.9 & 34.0 & 2.0\% & 4.5\% \\
% Local (int=8), 7 ref & 34.6 & 34.1 & 33.4 & 1.4\% & 3.5\% \\
% Local (int=8), 15 ref & 32.8 & 32.4 & 31.9 & 1.2\% & 2.7\% \\
% \bottomrule
% \end{tabular}
% \caption{Robustness of temporal aggregation under frame perturbations measured by mAR.}
% \label{tab:temporal_mAR_perturbations}
% \end{table}

\chapter{Discussion}


\chapter{Conclusion and Future Work}
\label{ch:conclusion}

\chapter{Appendix}

\begin{table}[t]
\centering
\small
\begin{tabular}{p{0.62\linewidth}cccc}
\toprule
Model & Clean & Low & Mid & High \\
\midrule

\multicolumn{5}{l}{\textbf{Brightness Change}} \\
YOLOV++ Single-frame baseline & 17.3 & 17.2 & 17.0 & 16.4 \\
YOLOV++ Local (int=10), 3 ref & 17.7 & 17.3 & 17.2 & 16.5 \\
YOLOV++ Random (int=10), 7 ref & 18.0 & 18.2 & 17.9 & 17.2 \\
TransVOD++ Single-frame baseline & 18.4 & 18.3 & 18.2 & 17.6 \\
TransVOD++ Stride 1, 7 ref & 19.3 & 19.3 & 19.1 & 18.4 \\
TransVOD++ Stride 8, 3 ref & 18.7 & 18.7 & 18.4 & 17.6 \\
\midrule

\multicolumn{5}{l}{\textbf{Contrast Change}} \\
YOLOV++ Single-frame baseline & 17.3 & 17.4 & 16.9 & 16.2 \\
YOLOV++ Local (int=10), 3 ref & 17.7 & 17.5 & 17.1 & 16.4 \\
YOLOV++ Random (int=10), 7 ref & 18.0 & 18.2 & 17.9 & 17.2 \\
TransVOD++ Single-frame baseline & 18.4 & 18.3 & 17.7 & 17.3\\
TransVOD++ Stride 1, 7 ref & 19.3 & 19.1 & 18.1 & 17.7 \\
TransVOD++ Stride 8, 3 ref & 18.7 & 18.5 & 17.4 & 17.2 \\
\midrule

\multicolumn{5}{l}{\textbf{Defocus Blur}} \\
YOLOV++ Single-frame baseline & 17.3 & 16.5 & 10.5 & 6.1 \\
YOLOV++ Local (int=10), 3 ref & 17.7 & 16.8 & 10.8 & 6.1 \\
YOLOV++ Random (int=10), 7 ref & 18.0 & 17.6 & 11.0 & 6.4 \\
TransVOD++ Single-frame baseline & 18.4 & 17.8 & 15.9 & 13.1 \\
TransVOD++ Stride 1, 7 ref & 19.3 & 18.5 & 16.5 & 13.6 \\
TransVOD++ Stride 8, 3 ref & 18.7 & 18.0 & 16.0 & 13.0 \\
\midrule

\multicolumn{5}{l}{\textbf{Gaussian Noise}} \\
YOLOV++ Single-frame baseline & 17.3 & 17.2 & 13.8 & 8.2 \\
YOLOV++ Local (int=10), 3 ref & 17.7 & 17.2 & 13.8 & 8.2 \\
YOLOV++ Random (int=10), 7 ref & 18.0 & 18.0 & 14.4 & 8.5 \\
TransVOD++ Single-frame baseline & 18.4 & 18.3 & 16.5 & 14.8 \\
TransVOD++ Stride 1, 7 ref & 19.3 & 19.2 & 17.5 & 15.2 \\
TransVOD++ Stride 8, 3 ref & 18.7 & 18.6 & 16.7 & 14.6 \\
\midrule

\multicolumn{5}{l}{\textbf{Jpeg Compression}} \\
YOLOV++ Single-frame baseline & 17.3 & 15.2 & 10.9 & 7.6 \\
YOLOV++ Local (int=10), 3 ref & 17.7 & 15.2 & 11.1 & 7.7 \\
YOLOV++ Random (int=10), 7 ref & 18.0 & 15.9 & 11.5 & 8.0 \\
TransVOD++ Single-frame baseline & 18.4 & 17.6 & 16.3 & 15.4 \\
TransVOD++ Stride 1, 7 ref & 19.3 & 18.0 & 17.1 & 16.0 \\
TransVOD++ Stride 8, 3 ref & 18.7 & 17.8 & 16.4 & 15.5 \\
\midrule

\multicolumn{5}{l}{\textbf{Motion Blur}} \\
YOLOV++ Single-frame baseline & 17.3 & 14.6 & 10.9 & 4.7 \\
YOLOV++ Local (int=10), 3 ref & 17.7 & 14.9 & 11.0 & 4.8 \\
YOLOV++ Random (int=10), 7 ref & 18.0 & 15.7 & 11.3 & 5.0 \\
TransVOD++ Single-frame baseline & 18.4 & 17.4 & 14.9 & 10.2 \\
TransVOD++ Stride 1, 7 ref & 19.3 & 18.2 & 15.6 & 10.6 \\
TransVOD++ Stride 8, 3 ref & 18.7 & 17.8 & 15.1 & 10.3 \\
\midrule

\multicolumn{5}{l}{\textbf{Pixelation}} \\
YOLOV++ Single-frame baseline & 17.3 & 2.0 & 0.2 & 0.0 \\
YOLOV++ Local (int=10), 3 ref & 17.7 & 2.1 & 0.2 & 0.0 \\
YOLOV++ Random (int=10), 7 ref & 18.0 & 2.1 & 0.2 & 0.0 \\
TransVOD++ Single-frame baseline & 18.4 & 14.7 & 10.7 & 5.5 \\
TransVOD++ Stride 1, 7 ref & 19.3 & 15.5 & 11.2 & 5.7 \\
TransVOD++ Stride 8, 3 ref & 18.7 & 14.9 & 10.9 & 5.7 \\
\bottomrule
\end{tabular}

\caption{Robustness under image corruptions (mAP$\times$100). For each perturbation type, we report Clean and Low/Mid/High severity. Rows include each model's baseline and selected best-performing temporal aggregation settings.}
\label{tab:robustness_map_by_corruption}
\end{table}

% Remove following line for the final thesis.
%\input{intro.tex} % A short introduction to LaTeX.

\backmatter

% Declare the use of AI tools as mentioned in the statement of originality.
% Use either the English aitools or the German kitools.
\begin{aitools}
\todo{Ihr Text hier.}
\end{aitools}

\begin{kitools}
\todo{Enter your text here.}
\end{kitools}

% Use an optional list of figures.
\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
\cleardoublepage % Start list of tables on the next empty right hand page.
\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of algorithms.
\listofalgorithms
\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
\printindex % Rerun build-thesis or build-all when removing the index through commenting.

% Add a glossary.
\printglossaries % Rerun build-thesis or build-all when removing it through commenting.

% Add a bibliography.
\bibliographystyle{alpha}
\bibliography{intro}

\end{document}