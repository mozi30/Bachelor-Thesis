% Copyright (C) 2014-2025 by Thomas Auzinger <thomas@auzinger.name>

% Inside the square brackets (e.g., [draft,onside]), different options can be chosen:
% 'draft': Obtain debug information on the build.
% 'final': Remove all debug information in the final build.
% 'onside': Create document for onsided printing (without empty pages added by LaTeX).
% 'twoside': Create document for twosided printing, which adds empty pages to start chapters on odd pages.
\documentclass[final,twoside]{vutinfth}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Moritz Anton Zideck} % The author name without titles.
\newcommand{\thesistitle}{Impact of Temporal Context on Robustness in UAV-based Imagery} % The title of the thesis. The English version should be used, if it exists.

% Create the XMP metadata file for the creation of PDF/A compatible documents.
\begin{filecontents*}[overwrite]{\jobname.xmpdata}
\Author{\authorname}                                    % The author's name in the document properties.
\Title{\thesistitle}                                    % The document's title in the document properties.
\Language{en-GB}                                        % The document's language in the document properties. Select 'en-US', 'en-GB', or 'de-AT'.
\Keywords{a\sep list\sep of\sep keywords}               % The document's keywords in the document properties (separated by '\sep ').
\Publisher{TU Wien}                                     % The document's publisher in the document properties.
\Subject{Thesis}                                        % The document's subject in the document properties.
\end{filecontents*}

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesetting of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}        % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes}  % Provides tooltip-like todo notes.
\usepackage{morewrites} % Increases the number of external files that can be used.
\usepackage[a-2b,mathxmp]{pdfx}      % Enables PDF/A compliance. Loads the package hyperref and has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists of acronyms. This package has to be included last.

\usepackage{float}
\usepackage{colortbl}
\usepackage{makecell}
% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around hyperlinks (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph indentation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setadvisor{Senior Lecturer Dipl.-Ing. Dr.techn.}{Sebastian Zambanini}{}{male}

% For bachelor and master theses:
\setfirstassistant{Dipl. Inf}{Marvin Burges}{}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{12217036}
\setdate{01}{01}{2001} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{Auswirkungen des zeitlichen Kontexts auf die Robustheit in der UAV-gestützten Bildverarbeitung} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor.
% Bachelor:
\setthesis{bachelor}
%
% Master:
%\setthesis{master}
%\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.

% For bachelor and master:
\setcurriculum{Media Informatics and Visual Computing}{Medieninformatik und Visual Computing} % Sets the English and German name of the curriculum.

% Optional reviewer data:
\setfirstreviewerdata{Affiliation, Country}
\setsecondreviewerdata{Affiliation, Country}

% Define colors
\definecolor{tableheader}{RGB}{220,220,220}
\definecolor{tablerow}{RGB}{245,245,245}



\begin{document}

\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

\addtitlepage{naustrian} % German title page.
\addtitlepage{english} % English title page.
\addstatementpage{naustrian}  % Select 'english' for the English statement text.

\begin{danksagung*}
\todo{Ihr Text hier.}
\end{danksagung*}

\begin{acknowledgements*}
\todo{Enter your text here.}
\end{acknowledgements*}

\begin{kurzfassung}
\todo{Ihr Text hier.}
\end{kurzfassung}

\begin{abstract}
\todo{Enter your text here.}
\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

\chapter{Introduction}
Video object detection is a relatively new field in computer vision that aims to leverage the temporal context of videos in order to improve detection performance compared to image-based object detection.
Temporal context refers to information that can be extracted from the time domain of a video, such as object motion, object trajectories, and changes in object appearance over time.
By exploiting this additional information, video object detection models can achieve higher accuracy, improved robustness, and greater temporal consistency when detecting objects across frames.
This is especially important in scenarios where objects may be occluded, blurred, or undergo significant appearance changes over time.
A prominent example is UAV-based imagery, where the camera is constantly moving, objects are often small, and visual conditions can change rapidly, making reliable object detection particularly challenging.

To date, most evaluations of object detection models—both image-based and video-based—focus primarily on accuracy measured on clean and unperturbed data.
However, in real-world deployments, and especially in UAV-based applications, visual data is frequently affected by a wide range of perturbations, including sensor noise, compression artifacts, motion blur, illumination changes, and variations in viewpoint and scale.
These perturbations can substantially degrade detection performance.
Although several works have investigated the robustness of image-based object detectors to such corruptions, only limited attention has been given to the robustness of video object detection models, and virtually no systematic studies exist for UAV-based video data in particular.

In this thesis, the focus is therefore placed on the robustness of video object detection models under common perturbations in UAV-based imagery.
Robustness is defined as the ability of a model to maintain its detection performance when exposed to adverse conditions such as lighting changes, weather effects, motion blur, occlusions, and variations in object appearance and scale.
A model may achieve high accuracy on clean data, yet still be unreliable in practice if its performance deteriorates significantly under realistic perturbations.
For safety-critical and autonomous UAV applications, such robustness is essential.

The central research question of this thesis is how the incorporation of temporal context in video object detection models affects their robustness to common perturbations in UAV-based imagery.
In particular, the impact of the number of reference frames used to construct the temporal context is analyzed. To address this question, a comprehensive evaluation of state-of-the-art video object detection models is conducted on a benchmark dataset designed for UAV scenarios.
In addition, a novel robustness evaluation metric is proposed, which quantifies the performance degradation of a model under different perturbations relative to its performance on clean data.

\chapter{Method}

\section{Models}
For this thesis two video detector models are chosen, which represent different approaches to leverage temporal context in different ways.
On the one hand there is TransVOD \cite{zhu2021transvod}, which is a transformer based model that uses attention mechanisms to aggregate temporal information from multiple frames.
On the other hand YOLOV \cite{liu2023yolov}, which is a one-stage detector that extends the popular YOLO architecture to video data by incorporating temporal feature fusion techniques.
For both models the Swin base backbone \cite{swin2021transformer} has been chosen, to ensure a fair compairson as well as petrain weights being available for both models.

%Write better
Together these models provide a good basis for evaluation, as they represent different design philosophies and represent pro and cons in terms of temporal context utilization, computational efficiency and detection accuracy.
\subsection{Transvod}
Transvod \cite{zhu2021transvod} is a end to end video object detection model base on DETR \cite{carion2020endtoendobjectdetectiontransformers}.
End to end mean that no hand crafted features as well as no post processing is needed, everthing is learned by the model itself.
The models fist version was proposed in 2021 as one of the first transformer based video object detection models to streamline the detection pipeline and remove the need for hand crafted features.
By encoding not only spatial but also temporal information in their attention mechanism, the model shows strong performance on various video object detection benchmarks.
In the most well known video object detection benchmark, ImageNet VID \cite{russakovsky2015ImageNet} outperforms its single frame baseline by 3.6 mAP(\%) achieving 80.7 mAP(\%) on the validation set.
One year later an improved version of TransVOD was proposed, called TransVOD++ \cite{zhu2022transvod++}, which builds upon the original TransVOD architecture and introduces several enhancements to further improve detection performance.
The main goal of TransVOD++ is to address the heavy computation costs as well as increase the detection accuracy of its predecessor.
Next to architectural improvements, which i will describe in the following, a new backbone namely Swin-Base \cite{swin2021transformer} instead of ResNet-101 \cite{he2016ResNet} was used to further boost performance.
With these improvements TransVOD++ was the first model to achieve over 90 mAP(\%) on the ImageNet VID validation set, reaching 90.0 mAP(\%).
TransVod short summary: ...
\subsubsection{Model design}


\subsection{YOLOV}
Base on the popular YOLO \cite{redmon2016yolo} architecture, to be more precise YOLOX \cite{yolox2021}, which are one-stage detectors known for their speed and efficiency, YOLOV \cite{liu2023yolov} extends this architecture to video data by incorporating temporal feature fusion techniques.
The paper that was published in 2023, was able to surpass previous state of the art video object detection models on the ImageNet VID \cite{russakovsky2015ImageNet} benchmark with a mAP(\%) of 85.5 on the validation set, while being still near real time capable with 22.7 FPS on a Nvidia TITAN RTX GPU.
YOLOV achieves this by introducing a temporal feature fusion module that aggregates features from multiple frames, allowing the model to leverage temporal context effectively.
Futhermore like TransVOD a improved version of YOLOV was proposed called YOLOV++ \cite{shi2024yolovpp}, which further enhances the temporal feature fusion mechanism and introduces additional optimizations to improve detection accuracy and efficiency.
The now improved YOLOV++ was able to achieve a new record mAP(\%) of 93.2 on the ImageNet VID validation set, while still being over 30 FPS on a Nvidia RTX 3090. 
However for this a special backbone names FocalNet-Large \cite{yang2022FocalModulation} is used.
Using the same Swin-Base \cite{swin2021transformer} backbone as for TransVOD++, YOLOV++ achieves a mAP(\%) of 90.7 on the ImageNet VID validation set, which is still significantly higher than TransVOD++ with 90.0 mAP(\%).
\subsubsection{Model design}



\section{Visdrone Dataset}
The Visdrone dataset \cite{zhu2021detection} is a large-scale dataset for different detection scenarios based on drone based imagery.
The images and videos were captured by various drone platforms in different urban and suburban areas of 14 different cities across China.
The objects are entitys of public street scenes, e.g., pedestrians, vehicles, bicycles, etc.
All together there are 10 different object categories.
It is curated by the \emph{AISKYEYE} research group from the \emph{Tianjin University in China}.
For this thesis VISDRONE2019-VID is used, to leverage the temporal context of the videos for the object detection task. 
All together the dataset contains 79 sequences with 33,366 frames, which are split 56 videos with 24,198 frames for training, 7 videos with 2,846 frames for validation and 16 videos with 6,322 frames for testing.
The dataset is chosen because of its large size and the challenging scenarios, e.g., different weather conditions, various altitudes and camera angles as well as high density of objects in the images.
Object sizes vary significantly, ranging from very small objects with only a few pixels to large objects covering a significant portion of the image.
This large difference in object sizes makes it also suitable to evaluate the performance of detection models under different perturbations and across different scales.

Since the dataset includes ignored regions, which are areas in the images where objects are not annotated due to beeing too crowded or too small, these regions are taken out of the evaluation to avoid penalizing the models for false positives in these areas.
This was done by blacking out these regions in the images before feeding them into the models for training and evaluation.
The method was chosen due to its simplicity and effectiveness. 
Other methods such as removing the annotations for these regions or masking them out during evaluation were considered, but blacking them out directly in the images was found to be the most straightforward approach.
Without this step models tend to produce a high number of false positives in these ignored regions, which would skew the evaluation results.
%show original and blacked out image
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/blackout/original.jpg}
        \caption{Original Image}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/blackout/blackout.jpg}
        \caption{Image with Ignored Regions Blacked Out}
    \end{subfigure}
    \caption{Example of an image from the Visdrone dataset with ignored regions blacked out for training and evaluation.}
    \label{fig:visdrone_ignored_regions}
\end{figure}




%exmaple images from the dataset
\begin{figure}[htbp]
    \centering

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{graphics/visdrone_example_1.jpg}
        \caption{Example 1}
    \end{subfigure}

    \par\medskip

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{graphics/visdrone_example_2.jpg}
        \caption{Example 2}
    \end{subfigure}

    \par\medskip

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{graphics/visdrone_example_3.jpg}
        \caption{Example 3}
    \end{subfigure}

    \caption{Example images from the Visdrone dataset.}
    \label{fig:visdrone_examples}
\end{figure}

\chapter{Perurbation}
A signficant part of this thesis to evaluate the robustness of video object detection models under common perturbations in UAV-based imagery.
In this sentence two key concepts must be defined: what robustness means in the context of an object detection model, and which perturbations commonly occur in UAV-based imagery.

\subsection{Definition of Robustness}
A widely used definition of robustness was proposed by Hendrycks and Dietterich~\cite{hendrycks2019benchmarking}, who define robustness as a model’s ability to maintain predictive performance under distribution shifts caused by common, naturally occurring image corruptions and perturbations.
In the context of object detection, this means that a robust model should be able to accurately detect and localize objects even when the input images are affected by various types of noise, distortions, or other adverse conditions.
In the context of this thesis, the main metric to quantify robustness is the relative performance degradation of a model mean avarage precision (mAP) under different perturbations compared to its performance on clean data.
\subsection{Common Perturbations in UAV-based Imagery}
The paper by Hendrycks and Dietterich~\cite{hendrycks2019benchmarking} introduced a benchmark suite called ImageNet-C, which consists of 15 different types of common image corruptions applied to the ImageNet dataset.
For further insight a paper named \emph{Benchmarking the Robustness of UAV Tracking Against Common Corruptions} \cite{liu2024UAVRobustness} which was published in 2024, is used to identify perturbations that commonly occur in UAV-based imagery.
Based on these works, the following perturbations are considered in this thesis:
\begin{itemize}
    \item Gaussian Noise: Random noise following a Gaussian distribution is added to the image pixels, simulating sensor noise.
    \item Motion Blur: Simulates the effect of camera or object motion during exposure, resulting in blurred images.
    \item Defocus Blur: Simulates the effect of an out-of-focus lens, resulting in blurred images.
    \item Brightness Changes: Adjusts the overall brightness of the image, simulating different lighting conditions.
    \item Contrast Changes: Adjusts the contrast of the image, affecting the distinction between light and dark areas.
    \item Jpeg Compression: Simulates artifacts introduced by JPEG compression at various quality levels.
\end{itemize}
The simulation of weather conditions such as fog, rain, and snow is not considered in this thesis, as these perturbations require more complex rendering techniques.
For each perturbation type, multiple severity levels are defined to assess robustness across a range of adverse conditions; in this thesis, three levels are used: low, medium, and high.
\subsection{Implementation}
To apply the defined perturbations to the Visdrone dataset, a custom data augmentation pipeline is implemented into to the models data loader.
Based on evaluation input parameters, the data loader applies the specified perturbation with the desired severity level to each frame before it is fed into the model for inference.
For more in depth evaluation a probability parameter is added, which defines the likeliness each perturbation being applied to a frame.

\paragraph{Gaussian noise.}
We add i.i.d.\ Gaussian noise:
\begin{equation}
\tilde{I} = \mathrm{clip}\big(I + N,\ 0,\ 255\big), \qquad
N_{h,w,c} \sim \mathcal{N}\!\left(0,\ (\sigma \cdot 255)^2\right),
\end{equation}
where $\sigma$ is the noise standard deviation.

\paragraph{Defocus blur.}
We approximate defocus blur by convolving the image with a normalized disk (pillbox) kernel:
\begin{equation}
\tilde{I} = I * K_{\mathrm{disk}}, \qquad
K_{\mathrm{disk}}(u,v)=\frac{1}{Z}\,\mathbb{1}\!\left(u^2+v^2 \le r^2\right),
\end{equation}
where $K_{\mathrm{disk}}$ is a $k\times k$ kernel, $r=\lfloor k/2 \rfloor$, $Z=\sum_{u,v} \mathbb{1}(\cdot)$, and $*$ denotes 2D convolution.

\paragraph{Motion blur.}
We simulate linear motion blur by convolving with a sparse line kernel of size $k\times k$ oriented by an angle $\theta$ (in degrees, default $\theta=0$). The kernel is constructed by placing ones on the discrete line
\begin{equation}
v = \tan(\theta)\,u, \qquad u \in \left[-\left\lfloor k/2\right\rfloor,\ \left\lfloor k/2\right\rfloor\right],
\end{equation}
rasterized onto the kernel grid and normalized to sum to one, then $\tilde{I} = I * K_{\mathrm{motion}}$.

\paragraph{Brightness change.}
We apply a global multiplicative gain:
\begin{equation}
\tilde{I} = \mathrm{clip}\big(\alpha I,\ 0,\ 255\big),
\end{equation}
with $\alpha>0$.

\paragraph{Contrast change.}
We scale deviations from the per-channel mean:
\begin{equation}
\mu_c = \frac{1}{HW}\sum_{h,w} I_{h,w,c}, \qquad
\tilde{I}_{h,w,c} = \mathrm{clip}\big((I_{h,w,c}-\mu_c)\alpha + \mu_c,\ 0,\ 255\big),
\end{equation}
with contrast factor $\alpha$.

\paragraph{Pixelation.}
We downsample and upsample the image using a block factor $p$ (default $p=8$). Specifically, we resize $I$ to $(\lfloor W/p\rfloor, \lfloor H/p\rfloor)$ using bilinear interpolation, then resize back to $(W,H)$ using nearest-neighbor interpolation:
\begin{equation}
\tilde{I} = \mathrm{NN}\big(\mathrm{BL}(I; \lfloor W/p\rfloor,\lfloor H/p\rfloor);\ W,H\big),
\end{equation}
where $\mathrm{BL}$ denotes bilinear resize and $\mathrm{NN}$ denotes nearest-neighbor resize.

\paragraph{JPEG compression.}
We simulate compression artifacts by encoding and decoding the image using JPEG with quality parameter $q$:
\begin{equation}
\tilde{I} = \mathrm{JPEGdecode}\big(\mathrm{JPEGencode}(I; q)\big).
\end{equation}

The specific severity levels are defined as follows:


\begin{table}[htbp]
\centering
\caption{Perturbation presets and severity levels used in robustness evaluation.}
\rowcolors{2}{tablerow}{white}
\begin{tabular}{l c c c}
\toprule
\rowcolor{tableheader}
\textbf{Perturbation} & \textbf{Low} & \textbf{Medium} & \textbf{High} \\
\midrule
Gaussian noise
& $\sigma = 0.01$
& $\sigma = 0.05$
& $\sigma = 0.10$ \\
Defocus blur
& $k = 3$
& $k = 7$
& $k = 11$ \\
Motion blur
& $k = 3,\ \theta = 0^\circ$
& $k = 7,\ \theta = 0^\circ$
& $k = 15,\ \theta = 0^\circ$ \\
Brightness change
& $\alpha = 1.10$
& $\alpha = 1.25$
& $\alpha = 1.45$ \\
Contrast change
& $\alpha = 1.10$
& $\alpha = 1.25$
& $\alpha = 1.45$ \\
Pixelation
& $p = 2$
& $p = 4$
& $p = 6$ \\
JPEG compression
& $q = 85$
& $q = 55$
& $q = 25$ \\
\bottomrule
\end{tabular}
\end{table}

With this setup, all together 18 different perturbation configurations (6 perturbation types $\times$ 3 severity levels) can be evaluated to assess the robustness of video object detection models in UAV-based imagery.
To give an impression of the applied perturbations, example images for each perturbation type and severity level are shown in Figure~\ref{fig:perturbation_examples}.

\begin{figure}[htbp]
\centering
\caption{Example images illustrating perturbation severities (Low, Medium, High).}
\label{fig:perturbation_examples}

\setlength{\tabcolsep}{2pt} % spacing between images
\renewcommand{\arraystretch}{1.0}

\begin{tabular}{l c c c}
\toprule
\textbf{Perturbation} & \textbf{Low} & \textbf{Medium} & \textbf{High} \\
\midrule

Gaussian noise
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/gaussian_noise_low.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/gaussian_noise_mid.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/gaussian_noise_high.jpg} \\

Defocus blur
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/defocus_blur_low.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/defocus_blur_mid.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/defocus_blur_high.jpg} \\

Motion blur
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/motion_blur_low.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/motion_blur_mid.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/motion_blur_high.jpg} \\

Brightness change
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/brightness_change_low.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/brightness_change_mid.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/brightness_change_high.jpg} \\

Contrast change
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/contrast_change_low.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/contrast_change_mid.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/contrast_change_high.jpg} \\

Pixelation
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/pixelation_low.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/pixelation_mid.jpg}
& \includegraphics[width=0.22\linewidth]{graphics/visdrone_perturbated_examples/pixelation_high.jpg} \\

\bottomrule
\end{tabular}
\end{figure}

\chapter{Evaluation Metric}
For robustness evaluation next to standard object detection metrics such as mean average precision (mAP) and mean average recall (mAR), a proper metric is needed, that quantifies the performance degradation of a model under different perturbations relative to its performance on clean data.
Derived from the papers \emph{Benchmarking the Robustness of UAV Tracking Against Common Corruptions} \cite{liu2024UAVRobustness} mCE (mean corruption error) which was based on the top-1 error rate, a new metric named \emph{mean Corruption Average Precision} (mCAP) and \emph{relative mean Corruption Average Precision} (rmCAP) is proposed.
Parallel to that \emph{mean Corruption Average Recall} (mCAR) and \emph{relative mean Corruption Average Recall} (rmCAR) is defined.

\paragraph{Mean Corruption Average Precision (mCAP)} is defined as: 

\begin{equation}
\mathrm{mCAP}
=
\frac{1}{|C|}
\sum_{c \in C}
\frac{1}{S}
\sum_{s=1}^{S}
\mathrm{mAP}_{s,c},
\end{equation}

where $C$ is the set of all perturbation configurations (i.e., perturbation types and severity levels), $S$ is the number of severity levels, and $\mathrm{mAP}_{s,c}$ is the mean average precision of the model under perturbation configuration $c$ at severity level $s$.

\paragraph{Relative mean Corruption Average Precision (rmCAP)} is defined as:
\begin{equation}
\mathrm{rmCAP}
=
\frac{1}{|C|}
\sum_{c \in C}
\frac{1}{S}
\sum_{s=1}^{S}
\frac{\mathrm{mAP}_{s,c}}{\mathrm{mAP}_{\mathrm{clean}}},
\end{equation}

where $\mathrm{mAP}_{\mathrm{clean}}$ is the mean average precision of the model on clean, unperturbed data.

\paragraph{Probability-based mean Corruption Average Precision (p-mCAP).}
In addition to corruption-type robustness, we evaluate robustness with respect to the
probability of perturbation occurrence.
Let $P$ denote the set of perturbation probabilities applied independently per frame.
We define probability-based mean Corruption Average Precision (p-mCAP) as
\begin{equation}
\mathrm{p\text{-}mCAP}
=
\frac{1}{|P|}
\sum_{p \in P}
\frac{1}{S}
\sum_{s=1}^{S}
\mathrm{mAP}_{p,s},
\end{equation}
where $\mathrm{mAP}_{p,s}$ denotes the detection performance when each frame is perturbed
with probability $p$ at severity level $s$.

\paragraph{Relative probability-based mean Corruption Average Precision (rp-mCAP).}
Analogous to rmCAP, we define a relative probability-based robustness metric by normalizing
p-mCAP with respect to a baseline detector:
\begin{equation}
\mathrm{rp\text{-}mCAP}
=
\frac{1}{|P|}
\sum_{p \in P}
\frac{
\sum_{s=1}^{S} \mathrm{mAP}^{f}_{p,s}
}{
\sum_{s=1}^{S} \mathrm{mAP}^{\text{baseline}}_{p,s}
}.
\end{equation}



\chapter{Experiments Setup}

\subsection{Hardware and Software Environment}
The training as well as evaluation of the models was done on eather Nvidia RTX 3060 Ti or Nvidia RTX 3090ti GPUs.
This was due due to the fact that CUDA 11.3 was required by the TransVOD implementation, which is not supported by the newer GPUs such as the RTX 40 series.
Both models were implemented in PyTorch \cite{paszke2019pytorch} and trained using the AdamW \cite{loshchilov2017decoupled} optimizer.

\subsection{Reference Frame Sampling Strategy} 
Since both models leverage temporal context from multiple frames, a proper reference frame sampling strategy is needed to select the frames that will be used as input to the models.
As written in the original papers as well as implemented int the provided code repositories, TransVOD++ and YOLOV++ use inherently different sampling strategies.

\subsubsection{Transvod Sampling}
Following the sampling strategy of TransVOD++, each video is divided into 16 temporal intervals. 
For a given target frame, reference frames are selected by sampling one frame from each interval. 
When the number of reference frames $N\geq8$, sampling is performed only on one side of the target frame; when $N\geq8$ reference frames are sampled from both the past and future relative to the target frame.
As a result, the selected reference frames are approximately evenly spaced in time around the target frame.

However, as will be discussed later, this global sampling strategy does not achieve reasonable performance on the VisDrone dataset.
To address this issue, we adopt a modified, more local sampling strategy in which reference frames are selected using a fixed temporal offset relative to the target frame.
Specifically, we test reference frames sampling with a temporal offset of 1 frame and 8 frames.
Overall, one single-frame baseline and twelve video-based TransVOD++ configurations were evaluated, combining three reference-frame sampling strategies with four different numbers of reference frames.
\subsubsection{YOLOV Sampling}
In the YOLOV++ paper, it is mentioned that test have resulted in better performance when using random sampling of reference frames in the whole video clip, called global sampling.
Therefore, for YOLOV++ the original global sampling strategy is used, where reference frames are sampled from the entire video clip.
To test the impact of local sampling on YOLOV++, an additional configuration with local sampling is evaluated, where reference frames are selected using a fixed temporal offset relative to the target frame with stride 10.
Therefore, a total of seven YOLOV++ configurations were evaluated, consisting of one single-frame baseline and six video-based variants formed by combining two reference-frame sampling strategies with three different reference-frame counts.


\subsection{Input Resolution}
Since the VisDrone dataset contains a large proportion of small objects, which are particularly challenging for object detectors, the input resolution must be chosen carefully. Higher resolutions preserve fine details and improve small-object detectability, but they also increase memory consumption and thus limit the feasible batch size on the available GPUs. 
Consequently, an input resolution of 960x544 pixels was chosen as a compromise between detection performance and memory efficiency, as well as keeping the original aspect ratio of the images.

\subsection{Training Protocol}
Both models are trained under an identical training protocol to ensure a fair comparison. Differences between the two models are limited to architectural components and are detailed separately.
Since both models already provided pretrained weights on the ImageNet VID \cite{russakovsky2015ImageNet} dataset with a Swin-Base \cite{swin2021transformer} backbone, these weights were used to finetune the single frame version of the models on the Visdrone2019-VID \cite{zhu2021detection} training set.
\begin{table}[htbp]
\centering
\caption{Fine-tuning hyperparameters for the single-frame detectors.}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{4pt}

\begin{tabular}{p{0.22\textwidth} p{0.35\textwidth} p{0.35\textwidth}}
\toprule
\rowcolor{tableheader}
\textbf{Hyperparameter} & \multicolumn{1}{c}{\textbf{YOLOV++}} & \multicolumn{1}{c}{\textbf{TransVOD++}} \\
\midrule
Batch size     & \multicolumn{1}{c}{$8$} & \multicolumn{1}{c}{$8$} \\
Precision      & \multicolumn{1}{c}{FP16} & \multicolumn{1}{c}{FP16} \\
Learning rate  & \multicolumn{1}{c}{$0.01/64 \approx 1.56 \cdot 10^{-4}$} & \multicolumn{1}{c}{\makecell[c]{$10^{-4}$ and $10^{-5}$ for backbone,\\ with lr drops after 3 and 5 epochs}} \\
Weight decay   & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} \\
Data augmentation
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
    \item Multi-scale resize: original size $\pm 64$ px
    \item Color jitter
    \item Geometric transforms: rotation $\pm 5^\circ$, translation $0.1$, shear $2^\circ$
\end{itemize}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
    \item Multi-scale resize: original size $\pm 64$ px
    \item Color jitter
\end{itemize}
\end{minipage}
\\
\bottomrule
\end{tabular}
\end{table}

After finetuning the single frame models, the video versions were trained by loading the finetuned single frame weights and training the temporal context modules while keeping the backbone and single frame detection head frozen.
For TransVOD++ the number of reference frames was used namely 1, 3, 7 and 15 reference frames.
However due to memory constraints only up to 7 reference frames could be used for YOLOV++ since the number of reference frame directly corresponds to the batch size.
This means that for 15 reference frames a batch size of 16 would be required, which surpasses the available GPU memory of 24GB of the NVIDIA RTX 3090ti.

For training the video models, different hyperparameters were used compared to the single frame models, which are summarized in the following table.
\begin{table}[htbp]
\centering
\caption{Fine-tuning hyperparameters for the video detectors.}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{4pt}

\begin{tabular}{p{0.22\textwidth} p{0.35\textwidth} p{0.35\textwidth}}
\toprule
\rowcolor{tableheader}
\textbf{Hyperparameter} & \multicolumn{1}{c}{\textbf{YOLOV++}} & \multicolumn{1}{c}{\textbf{TransVOD++}} \\
\midrule
Batch size     & \multicolumn{1}{c}{number of reference frames} & \multicolumn{1}{c}{$1$} \\
Precision      & \multicolumn{1}{c}{FP16} & \multicolumn{1}{c}{FP16} \\
Learning rate  & \multicolumn{1}{c}{$10^{-5}$} & \multicolumn{1}{c}{\makecell[c]{$2 \cdot 10^{-6}$}} \\
Weight decay   & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} & \multicolumn{1}{c}{$1 \cdot 10^{-4}$} \\
Data augmentation
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
\end{itemize}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}\vspace{0pt}
\begin{itemize}[leftmargin=*,nosep]
    \item Random horizontal flip
\end{itemize}
\end{minipage}
\\
\bottomrule
\end{tabular}
\end{table}

All together the single frame as well as 3 different video models for YOLOV++ and 4 different video models for TransVOD++ were trained and evaluated.

For all the evaluttions random seeds were fixe to 42 to ensure reproducibility of the results, except for random sample selection of reference frames during training, where different random seeds were used to increase the diversity of training samples.
To see if this had an impact on the results, 5 different training runs were done for the YOLOV++ model with 7 reference frames, which showed almost no variation in mAP(\%) of less than 0.3\% between the runs.

For evaluation the models were first evaluated on the clean Visdrone2019-VID \cite{zhu2021detection} validation set to obtain the baseline mAP(\%) and mAR(\%) values.
Subsequently the models were evaluated on the perturbed versions of the validation set for each perturbation type and severity level as described in the \emph{Perturbation} chapter.
Futhermore evaluations with different perturbation probabilities were conducted to assess the models robustness when only a subset of frames are perturbed.
For this two different probabilities were chosen namely 0.5 and 0.75 meaning that each frame has a 50\% or 75\% chance of being perturbed.
The goal of this evaluation is to simulate more realistic scenarios where not all frames are affected by perturbations and conclude on how well the models can leverage unperturbed frames to maintain detection performance.
\chapter{Results}
\section{Clean Results}
To form a basis for the robustness evaluation, the models were first evaluated on the clean Visdrone2019-VID \cite{zhu2021detection} validation set.
The results are summarized in Table~\ref{tab:clean_results}.
\begin{table}[htbp]
\centering
\caption{Detection performance of all evaluated video object detection models on the VisDrone2019-VID validation set. (Best results in \textbf{bold} and worst results in \underline{underline} per model.)}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{6pt}

\begin{tabular}{p{0.50\textwidth} c c}
\toprule
\rowcolor{tableheader}
\textbf{Model Configuration} & \textbf{mAP$_{50:95}$ (\%)} & \textbf{mAR$_{50:95}$ (\%)} \\
\midrule
\multicolumn{3}{l}{\textbf{YOLOV++}} \\
Single-frame baseline                 & \underline{17.2} & \underline{28.3} \\
Sampling random, 1 reference frame         & 17.5 & 29.2 \\
Sampling random, 3 reference frames        & 17.8 & 29.7 \\
Sampling random, 7 reference frames        & \textbf{18.0} & \textbf{30.0} \\
Sampling local, 1 reference frame         & 17.3 & 29.0 \\
Sampling local, 3 reference frames        & 17.7 & 29.5 \\
Sampling local, 7 reference frames        & 17.8 & \textbf{30.0} \\
\midrule
\multicolumn{3}{l}{\textbf{TransVOD++}} \\
Single-frame baseline                 & 19.4 & 34.8 \\
Sampling stride 1, 1 reference frame         & 19.6 & 35.8 \\
Sampling stride 1, 3 reference frames        & 20.0 & 36.4 \\
Sampling stride 1, 7 reference frames        & \textbf{20.4} & \textbf{36.3} \\
Sampling stride 1, 15 reference frames       & 19.8 & 35.2 \\
Sampling stride 8, 1 reference frame         & 19.3 & 35.2 \\
Sampling stride 8, 3 reference frames        & 19.7 & 35.6 \\
Sampling stride 8, 7 reference frames        & 19.4 & 34.6 \\
Sampling stride 8, 15 reference frames       & 18.2 & 32.8 \\
Sampling global, 1 reference frame         & 19.5 & 35.1 \\
Sampling global, 3 reference frames        & 19.3 & 34.8 \\
Sampling global, 7 reference frames        & 18.9 & 33.3 \\
Sampling global, 15 reference frames       & \underline{17.2} & \underline{31.0} \\
\bottomrule
\end{tabular}
\label{tab:clean_results}
\end{table}

As can it can be seen in Table~\ref{tab:clean_results}, the single frame baseline of TransVOD++ outperforms the single frame baseline of YOLOV++ by a significant margin of 2.2\% mAP and 6.5\% mAR.
When looking at the video-based configurations, both models show improvements over their respective single frame baselines.
\subsection{YOLOV++ clean data results}
For YOLOV++, the best performance is achieved using random sampling of reference frames, with 7 reference frames.
This configuration yields the highest mAP of 18.0\% and mAR of 30.0\%, therefore an increase of 0.8\% mAP and 1.7\% mAR or 4.7\% and 6.0\% relative improvement compared to the single frame baseline.
Alligned with the findings in the YOLOV++ paper, random sampling of reference frames outperforms local sampling in all configurations, however the performance gap is relatively small with only 0.2\% mAP and 0.0-0.5\% mAR difference between the two sampling strategies.

\subsection{TransVOD++ clean data results}
For TransVOD++, the best performance is achieved using a local sampling strategy with a temporal stride of 1 frame and 7 reference frames.
This configuration yields the highest mAP of 20.4\% and mAR of 36.3\%, therefore an increase of 1.0\% mAP and 1.5\% mAR or 5.2\% and 4.3\% relative improvement compared to the single frame baseline.
However, it is important to note that in contrast to the orginal TransVOD++ paper, the global sampling strategy results in the worst performance on the Visdrone dataset.
It even performs worse than the single frame baseline when using 15 reference frames, which indicates that this sampling strategy is not suitable for the Visdrone dataset.
FIND EXPLENATION FOR DEGRADING PERFORMANCE WITH GLOBAL SAMPLING
To get a better insight into the impact of the sampling strategy, local sampling with a temporal stride of 8 frames was also evaluated.
This configuration shows a similar trend as global sampling, where the performance degrades with an increasing number of reference frames.
This further supports the hypothesis that sampling frames that are temporally distant from the target frame is not beneficial for the Visdrone dataset, likely due to the fast motion and rapid scene changes in UAV-based videos.

Since the global sampling strategy achieves the worst performance with a significant margin, it will not be considered for the robustness evaluation in the following chapter.
Therefore we we have a total of 9 instead of 13 different TransVOD++ configurations for robustness evaluation.



\section{Perturbation Results}

The results are summarized in Table~\ref{tab:clean_perturbated}.
\begin{table}[H]
\centering
\caption{Detection performance of all evaluated video object detection models on the VisDrone2019-VID validation set under perturbations. (Best results in \textbf{bold} and worst results in \underline{underline} per model.)}
\rowcolors{2}{tablerow}{white}

\small
\setlength{\tabcolsep}{4pt}

\begin{tabular}{p{0.40\textwidth} c c c c}
\toprule
\rowcolor{tableheader}
\textbf{Model Configuration} & \textbf{mCAP$_{50:95}$ (\%)} & \textbf{mCAR$_{50:95}$ (\%)} & \textbf{rmCAP} & \textbf{rmCAR} \\
\midrule
\multicolumn{5}{l}{\textbf{YOLOV++}} \\
Single-frame baseline                 & \underline{11.3} & \underline{20.1} & 0.66 & \underline{0.71} \\
Sampling local, 1 reference frame     & 11.4 & 21.7 & 0.66 & 0.75 \\
Sampling local, 3 reference frames    & 11.5 & 21.8 & 0.65 & 0.74 \\
Sampling local, 7 reference frames    & 11.4 & 21.8 & \underline{0.64} & 0.73 \\
Sampling random, 1 reference frame    & 11.7 & 22.1 & 0.67 & 0.76 \\
Sampling random, 3 reference frames   & 11.9 & 22.6 & 0.67 & 0.76 \\
Sampling random, 7 reference frames   & \textbf{12.0} & \textbf{23.0} & \textbf{0.67} & \textbf{0.77} \\
\midrule
\multicolumn{5}{l}{\textbf{TransVOD++}} \\
Single-frame baseline                 & 18.3 & 33.3 & 0.94 & 0.96 \\
Sampling stride 1, 1 reference frame  & 18.7 & 34.0 & 0.95 & 0.95 \\
Sampling stride 1, 3 reference frames & 18.9 & 34.5 & 0.95 & 0.95 \\
Sampling stride 1, 7 reference frames & \textbf{19.1} & \textbf{34.7} & 0.94 & 0.96 \\
Sampling stride 1, 15 reference frames& 18.5 & 34.0 & \underline{0.93} & \textbf{0.97} \\
Sampling stride 8, 1 reference frame  & 18.5 & 33.5 & \textbf{0.96} & 0.95 \\
Sampling stride 8, 3 reference frames & 18.5 & 33.7 & 0.94 & 0.95 \\
Sampling stride 8, 7 reference frames & 18.2 & 33.2 & 0.94 & 0.96 \\
Sampling stride 8, 15 reference frames& \underline{17.0} & \underline{31.2} & 0.93 & \underline{0.95} \\
\bottomrule
\end{tabular}
\label{tab:perturbated_results}
\end{table}

On perturbed data, TransVOD++ significantly outperforms YOLOV++ in terms of mCAP and mCAR across all configurations, as seen in Table~\ref{tab:perturbated_results}.
This performance gap can be noticed on every model configuration, with TransVOD++ achieving mCAP values ranging from 17.0\% to 19.1\%, while YOLOV++ achieves mCAP values between 11.3\% and 12.0\%.
Looking at the relative metrics rmCAP and rmCAR, TransVOD++ also demonstrates superior robustness to perturbations, on the other hand a signicant releative performance drop can be observed for YOLOV++.
\subsection{YOLOV++ perturbation results}
The perturbation results for YOLOV++ in Table~\ref{tab:perturbated_results} show that the video-based configurations outperform the single-frame baseline in terms of mCAP and mCAR.
This phenomenon is consistent with the clean data results \ref{tab:clean_results}, where video-based models also showed improved performance.
Looking at the the realtive metrics rmCAP and rmCAR, no clear trend can be observed between model configurations. 
This indicates that the number of reference frames and sampling strategy have little impact on the relative robustness of YOLOV++ to perturbations.
In general the YOLOV++ models show a signicant relative performance drop when evaluated on perturbed data, with rmCAP values ranging from 0.64 to 0.67 and rmCAR values between 0.71 and 0.77.
\subsection{TransVOD++ perturbation results}
For TransVOD++, a similar trend as in the clean data results can be observed in Table~\ref{tab:perturbated_results}, where the models gain from the first few reference frames but performance degrades when using too many reference frames.
Forthermore the local sampling stagegy with a temporal stride of 1 frame outperforms the stride of 8 frames strategy, with the best performance achieved using 7 reference frames.
However an important observation is that the relative robustness of all TransVOD++ configurations is shown a very strong robustness to perturbations, with rmCAP values ranging from 0.93 to 0.96 and rmCAR values between 0.95 and 0.97.
As with YOLOV++ no clear trend can be observed between model configurations, indicating that the number of reference frames and sampling strategy have little impact on the relative robustness of TransVOD++ to perturbations.

\subsection{Perturbations and Reference Frames}
Opposite of the initial hypothesis, increasing the number of reference frames does not necessarily lead to improved robustness against perturbations.
This trend is can be observed in both YOLOV++ and TransVOD++ models, while video models generally outperform their single-frame baselines, relative to their clean data performance, the robustness does not improve with more reference frames.
This indicates that simply adding more temporal context is not sufficient to enhance robustness against perturbations.
Therefore if all frames are perturbated, the models are not able to leverage information from multibiple perturbated frames to improve detection performance.

\section{Deeper insight into Perturbations}
To gain a deeper understanding of into model robustness in the following sections, the performance of the best YOLOV++ and TransVOD++ configurations on each perturbation type are analysed separately. 

\dots

\section{Perturbation Probability Results}
\dots 

\chapter{Conclusion and Future Work}
\label{ch:conclusion}



% Remove following line for the final thesis.
%\input{intro.tex} % A short introduction to LaTeX.

\backmatter

% Declare the use of AI tools as mentioned in the statement of originality.
% Use either the English aitools or the German kitools.
\begin{aitools}
\todo{Ihr Text hier.}
\end{aitools}

\begin{kitools}
\todo{Enter your text here.}
\end{kitools}

% Use an optional list of figures.
\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
\cleardoublepage % Start list of tables on the next empty right hand page.
\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of algorithms.
\listofalgorithms
\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
\printindex % Rerun build-thesis or build-all when removing the index through commenting.

% Add a glossary.
\printglossaries % Rerun build-thesis or build-all when removing it through commenting.

% Add a bibliography.
\bibliographystyle{alpha}
\bibliography{intro}

\end{document}